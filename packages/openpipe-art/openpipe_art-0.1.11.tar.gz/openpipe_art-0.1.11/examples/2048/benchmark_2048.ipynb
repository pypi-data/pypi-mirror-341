{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPipe client initialized\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from openpipe.client import OpenPipe\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "op_client = OpenPipe()\n",
    "print(\"OpenPipe client initialized\")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.utils.get_trajectory_messages import get_trajectory_messages\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "from litellm import acompletion\n",
    "\n",
    "from .utils import generate_game, render_board, apply_agent_move, max_cell_value, check_game_finished\n",
    "\n",
    "\n",
    "WINNING_VALUE = 512\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError, requests.ReadTimeout))\n",
    "async def rollout(\n",
    "    model: str, iteration: int, is_validation: bool\n",
    ") -> art.Trajectory:\n",
    "\n",
    "    game = generate_game()\n",
    "\n",
    "    move_number = 0\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\",\n",
    "            }\n",
    "        ],\n",
    "        reward=0,\n",
    "        metrics={\"test\": 5},\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "                    \n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = get_trajectory_messages(trajectory)\n",
    "\n",
    "        async def get_completion():\n",
    "            return await acompletion(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "        \n",
    "        try:\n",
    "            op_client.report(\n",
    "                requested_at=requested_at,\n",
    "                received_at=int(time.time() * 1000),\n",
    "                req_payload={\n",
    "                    \"model\": model,\n",
    "                    \"messages\": messages,\n",
    "                    \"metadata\": {\n",
    "                        \"game_id\": game[\"id\"],\n",
    "                        \"notebook-id\": \"2048\",\n",
    "                        \"iteration\": str(iteration),\n",
    "                        \"validation\": str(is_validation),\n",
    "                        \"move_number\": str(move_number),\n",
    "                    },\n",
    "                },\n",
    "                resp_payload=chat_completion,\n",
    "                status_code=200,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "            move_number += 1\n",
    "        except ValueError:\n",
    "            trajectory.reward = -1\n",
    "            break\n",
    "\n",
    "        if check_game_finished(game):\n",
    "            max_value = max_cell_value(game)\n",
    "\n",
    "            if max_value < WINNING_VALUE:\n",
    "                # scale reward logarithmically between 0 for 2 and 1 for 2048\n",
    "                trajectory.reward = (math.log(max_value, 2) - 1) / (math.log(WINNING_VALUE, 2) - 1)\n",
    "            else:\n",
    "                # double reward if it wins\n",
    "                trajectory.reward = 2\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        op_client.update_log_metadata(\n",
    "            filters=[\n",
    "                {\n",
    "                    \"field\": \"completionId\",\n",
    "                    \"equals\": last_completion.id,\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"reward\": str(trajectory.reward),\n",
    "                \"reward_assigned\": \"true\",\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb36e80e7e88497ba53130635b97e6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for gpt-4o-mini: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from art.utils.benchmark_rollout import benchmark_rollout\n",
    "\n",
    "await benchmark_rollout(\n",
    "    \"gpt-4o-mini\",\n",
    "    100,\n",
    "    rollout,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32132b9dee7942d395b3a28aafc64ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reporting to OpenPipe: Server disconnected without sending a response.\n",
      "Average reward for gpt-4o-2024-11-20: 0.70625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.70625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await benchmark_rollout(\n",
    "    \"gpt-4o-2024-11-20\",\n",
    "    100,\n",
    "    rollout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00518e37c624e489f2069da0e47dbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for o3-mini-2025-01-31: 0.7575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await benchmark_rollout(\n",
    "    \"o3-mini-2025-01-31\",\n",
    "    100,\n",
    "    rollout,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208be1f32a1949eaac3d13d9953cc8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Benchmarking rollout:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for gpt-4.5-preview-2025-02-27: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await benchmark_rollout(\n",
    "    \"gpt-4.5-preview-2025-02-27\",\n",
    "    10,\n",
    "    rollout,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

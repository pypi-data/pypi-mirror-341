{"title":"Getting Started with PyFixest","markdown":{"yaml":{"title":"Getting Started with PyFixest","format":{"html":{"html-table-processing":"none"}},"toc":true,"toc-title":"On this page","toc-location":"left"},"headingText":"OLS with Fixed Effects","containsRefs":false,"markdown":"\n\n\n## What is a fixed effect model?\n\nA fixed effect model is a statistical model that includes fixed effects, which are parameters that are estimated to be constant across different groups.\n\n**Example [Panel Data]:** In the context of panel data, fixed effects are parameters that are constant across different individuals or time. The typical model example is given by the following equation:\n\n$$\nY_{it} = \\beta X_{it} + \\alpha_i + \\psi_t + \\varepsilon_{it}\n$$\n\nwhere $Y_{it}$ is the dependent variable for individual $i$ at time $t$, $X_{it}$ is the independent variable, $\\beta$ is the coefficient of the independent variable, $\\alpha_i$ is the individual fixed effect, $\\psi_t$ is the time fixed effect, and $\\varepsilon_{it}$ is the error term. The individual fixed effect $\\alpha_i$ is a parameter that is constant across time for each individual, while the time fixed effect $\\psi_t$ is a parameter that is constant across individuals for each time period.\n\nNote however that, despite the fact that fixed effects are commonly used in panel setting, one does not need a panel data set to work with fixed effects. For example, cluster randomized trials with cluster fixed effects, or wage regressions with worker and firm fixed effects.\n\nIn this \"quick start\" guide, we will show you how to estimate a fixed effect model using the `PyFixest` package. We do not go into the details of the theory behind fixed effect models, but we focus on how to estimate them using `PyFixest`.\n\n## Read Sample Data\n\nIn a first step, we load the module and some synthetic example data:\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ntry:\n    from lets_plot import LetsPlot\n    _HAS_LETS_PLOT = True\nexcept ImportError:\n    _HAS_LETS_PLOT = False\n\nfrom marginaleffects import slopes, avg_slopes\n\nimport pyfixest as pf\n\nif _HAS_LETS_PLOT:\n    LetsPlot.setup_html()\n\nplt.style.use(\"seaborn-v0_8\")\n\n%load_ext watermark\n%config InlineBackend.figure_format = \"retina\"\n%watermark --iversions\n\ndata = pf.get_data()\n\ndata.head()\n```\n\n```{python}\ndata.info()\n```\n\nWe see that some of our columns have missing data.\n\n## OLS Estimation\n\nWe are interested in the relation between the dependent variable `Y` and the independent variables `X1` using a fixed effect model for `group_id`. Let's see how the data looks like:\n\n```{python}\nax = data.plot(kind=\"scatter\", x=\"X1\", y=\"Y\", c=\"group_id\", colormap=\"viridis\")\n```\n\nWe can estimate a fixed effects regression via the `feols()` function. `feols()` has three arguments: a two-sided model formula, the data, and optionally, the type of inference.\n\n```{python}\nfit = pf.feols(fml=\"Y ~ X1 | group_id\", data=data, vcov=\"HC1\")\ntype(fit)\n```\n\n\nThe first part of the formula contains the dependent variable and \"regular\" covariates, while the second part contains fixed effects.\n\n`feols()` returns an instance of the `Fixest` class.\n\n## Inspecting Model Results\n\nTo inspect the results, we can use a summary function or method:\n\n```{python}\nfit.summary()\n```\n\nOr display a formatted regression table:\n\n```{python}\npf.etable(fit)\n```\n\nAlternatively, the `.summarize` module contains a `summary` function, which can be applied on instances of regression model objects or lists of regression model objects. For details on how to customize `etable()`, please take a look at the [dedicated vignette](https://py-econometrics.github.io/pyfixest/table-layout.html).\n\n```{python}\npf.summary(fit)\n```\n\nYou can access individual elements of the summary via dedicated methods: `.tidy()` returns a \"tidy\" `pd.DataFrame`,\n`.coef()` returns estimated parameters, and `se()` estimated standard errors. Other methods include `pvalue()`, `confint()`\nand `tstat()`.\n\n```{python}\nfit.tidy()\n```\n\n```{python}\nfit.coef()\n```\n\n```{python}\nfit.se()\n```\n\n```{python}\nfit.tstat()\n```\n\n```{python}\nfit.confint()\n```\n\nLast, model results can be visualized via dedicated methods for plotting:\n\n```{python}\nfit.coefplot()\n# or pf.coefplot([fit])\n```\n\n## How to interpret the results?\n\nLet's have a quick d-tour on the intuition behind fixed effects models using the example above. To do so, let us begin by comparing it with a simple OLS model.\n\n```{python}\nfit_simple = pf.feols(\"Y ~ X1\", data=data, vcov=\"HC1\")\n\nfit_simple.summary()\n```\n\nWe can compare both models side by side in a regression table:\n\n```{python}\npf.etable([fit, fit_simple])\n```\n\nWe see that the `X1` coefficient is `-1.019`, which is less than the value from the OLS model in column (2). Where is the difference coming from?\nWell, in the fixed effect model we are interested in controlling for the feature `group_id`. One possibility to do this is by adding a simple dummy variable for each level of `group_id`.\n\n```{python}\nfit_dummy = pf.feols(\"Y ~ X1 + C(group_id) \", data=data, vcov=\"HC1\")\n\nfit_dummy.summary()\n```\n\nThis is does not scale well! Imagine you have 1000 different levels of `group_id`. You would need to add 1000 dummy variables to your model. This is where fixed effect models come in handy. They allow you to control for these fixed effects without adding all these dummy variables. The way to do it is by a *demeaning procedure*. The idea is to subtract the average value of each level of `group_id` from the respective observations. This way, we control for the fixed effects without adding all these dummy variables. Let's try to do this manually:\n\n```{python}\ndef _demean_column(df: pd.DataFrame, column: str, by: str) -> pd.Series:\n    return df[column] - df.groupby(by)[column].transform(\"mean\")\n\n\nfit_demeaned = pf.feols(\n    fml=\"Y_demeaned ~ X1_demeaned\",\n    data=data.assign(\n        Y_demeaned=lambda df: _demean_column(df, \"Y\", \"group_id\"),\n        X1_demeaned=lambda df: _demean_column(df, \"X1\", \"group_id\"),\n    ),\n    vcov=\"HC1\",\n)\n\nfit_demeaned.summary()\n```\n\nWe get the same results as the fixed effect model `Y1 ~ X | group_id` above. The `PyFixest` package uses a more efficient algorithm to estimate the fixed effect model, but the intuition is the same.\n\n## Updating Regression Coefficients\n\nYou can update the coefficients of a model object via the `update()` method, which may be useful in an online learning setting where data arrives sequentially.\n\nTo see this in action, let us first fit a model on a subset of the data:\n\n```{python}\ndata_subsample = data.sample(frac=0.5)\nm = pf.feols(\"Y ~ X1 + X2\", data=data_subsample)\n# current coefficient vector\nm._beta_hat\n```\n\nThen sample 5 new observations and update the model with the new data. The update rule is\n\n$$\n\\hat{\\beta}_{n+1} = \\hat{\\beta}_n + (X_{n+1}' X_{n+1})^{-1} x_{n+1} + (y_{n+1} - x_{n+1} \\hat{\\beta}_n)\n$$\n\nfor a new observation $(x_{n+1}, y_{n+1})$.\n\n```{python}\nnew_points_id = np.random.choice(list(set(data.index) - set(data_subsample.index)), 5)\nX_new, y_new = (\n    np.c_[np.ones(len(new_points_id)), data.loc[new_points_id][[\"X1\", \"X2\"]].values],\n    data.loc[new_points_id][\"Y\"].values,\n)\nm.update(X_new, y_new)\n```\n\nWe verify that we get the same results if we had estimated the model on the appended data.\n\n```{python}\npf.feols(\n    \"Y ~ X1 + X2\", data=data.loc[data_subsample.index.append(pd.Index(new_points_id))]\n).coef().values\n```\n\n# Standard Errors and Inference\n\nSupported covariance types are \"iid\", \"HC1-3\", CRV1 and CRV3 (up to two-way clustering).\n\n**Why do we have so many different types of standard errors?**\n\nThe standard errors of the coefficients are crucial for inference. They tell us how certain we can be about the estimated coefficients. In the presence of heteroskedasticity (a situation which typically arises with cross-sectional data), the standard OLS standard errors are biased. The `pyfixest` package provides several types of standard errors that are robust to heteroskedasticity.\n\n- `iid`: assumes that the error variance is spherical, i.e. errors are homoskedastic and not correlated (independent and identically distributed errors have a spherical error variance).\n- `HC1-3`: heteroskedasticity-robust standard errors according to White (1980) and MacKinnon and White (1985). See [Econometric Computing with HC and HAC\nCovariance Matrix Estimators](https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich.pdf) from the [`sandwich`](https://cran.r-project.org/web/packages/sandwich/) package for more details.\n- `CRV1` and `CRV3`: cluster robust standard errors according to Cameron, Gelbach, and Miller (2011). See [A Practitioner's Guide to Cluster-Robust Inference](https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf).  For   `CRV1` and `CRV3` one should pass a dictionaty of the form `{\"CRV1\": \"clustervar\"}`.\n\nInference can be adjusted \"on-the-fly\" via the `.vcov()` method:\n\n```{python}\nfit.vcov({\"CRV1\": \"group_id + f2\"}).summary()\n\nfit.vcov({\"CRV3\": \"group_id\"}).summary()\n```\n\nThe estimated covariance matrix is available as an attribute of the `Feols` object called `._vcov`.\n\n## Inference via the Wild Bootstrap\n\nIt is also possible to run a wild (cluster) bootstrap after estimation (via the [wildboottest module](https://github.com/py-econometrics/wildboottest), see [MacKinnon, J. G., Nielsen, M. Ø., & Webb, M. D. (2023). Fast and reliable jackknife and bootstrap methods for cluster-robust inference. Journal of Applied Econometrics, 38(5), 671–694.](http://qed.econ.queensu.ca/pub/faculty/mackinnon/working-papers/qed_wp_1485.pdf)):\n\n```{python}\nfit2 = pf.feols(fml=\"Y ~ X1\", data=data, vcov={\"CRV1\": \"group_id\"})\nfit2.wildboottest(param=\"X1\", reps=999)\n```\n\n## The Causal Cluster Variance Estimator\n\nAdditionally, `PyFixest` supports the causal cluster variance estimator following [Abadie et al. (2023)](https://academic.oup.com/qje/article/138/1/1/6750017). Let's look into it with another data set:\n\n```{python}\ndf = pd.read_stata(\"http://www.damianclarke.net/stata/census2000_5pc.dta\")\n\ndf.head()\n```\n\n\n```{python}\naxes = df.plot.hist(column=[\"ln_earnings\"], by=[\"college\"])\n```\n\nNow we can estimate the model `ln_earnings ~ college` where we cluster the standard errors at the state level:\n\n```{python}\nfit3 = pf.feols(\"ln_earnings ~ college\", vcov={\"CRV1\": \"state\"}, data=df)\nfit3.ccv(treatment=\"college\", pk=0.05, n_splits=2, seed=929)\n```\n\n## Randomization Inference\n\nYou can also conduct inference via randomization inference [(see Heß, Stata Journal 2017)](https://hesss.org/ritest.pdf).\n`PyFixest` supports random and cluster random sampling.\n\n```{python}\nfit2.ritest(resampvar=\"X1=0\", reps=1000, cluster=\"group_id\")\n```\n\n## Multiple Testing Corrections: Bonferroni and Romano-Wolf\n\nTo correct for multiple testing, p-values can be adjusted via either the [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction), the method by Romano and Wolf (2005), see for example [The Romano-Wolf Multiple Hypothesis\nCorrection in Stata](https://docs.iza.org/dp12845.pdf), and the method by Westfall & Young (see [here](https://www.jstor.org/stable/2532216)).\n\n```{python}\npf.bonferroni([fit, fit2], param=\"X1\").round(3)\n```\n\n```{python}\npf.rwolf([fit, fit2], param=\"X1\", reps=9999, seed=1234).round(3)\n```\n\n```{python}\npf.wyoung([fit, fit2], param=\"X1\", reps=9999, seed=1234).round(3)\n```\n\n## Joint Confidence Intervals\n\nSimultaneous confidence bands for a vector of parameters can be computed via the `joint_confint()` method. See [Simultaneous confidence bands: Theory, implementation, and an application to SVARs](https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2656) for background.\n\n```{python}\nfit_ci = pf.feols(\"Y ~ X1+ C(f1)\", data=data)\nfit_ci.confint(joint=True).head()\n```\n\n# Panel Data Example: Causal Inference for the Brave and True\n\nIn this example we replicate the results of the great (freely available reference!) [Causal Inference for the Brave and True - Chapter 14](https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-Fixed-Effects.html). Please refer to the original text for a detailed explanation of the data.\n\n```{python}\ndata_path = \"https://raw.githubusercontent.com/bashtage/linearmodels/main/linearmodels/datasets/wage_panel/wage_panel.csv.bz2\"\ndata_df = pd.read_csv(data_path)\n\ndata_df.head()\n```\n\nThe objective is to estimate the effect of the variable `married` on the variable `lwage` using a fixed effect model on the entity variable `nr` and the time variable `year`.\n\n```{python}\npanel_fit = pf.feols(\n    fml=\"lwage ~ expersq + union + married + hours | nr + year\",\n    data=data_df,\n    vcov={\"CRV1\": \"nr + year\"},\n)\n\npf.etable(panel_fit)\n```\n\nWe obtain the same results as in the book!\n\n# Instrumental Variables (IV) Estimation\n\nIt is also possible to estimate [instrumental variable models](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) with *one* endogenous variable and (potentially multiple) instruments.\n\nIn general, the syntax for IV is `depvar ~ exog.vars | fixef effects | endog.vars ~ instruments`.\n\n```{python}\niv_fit = pf.feols(fml=\"Y2 ~ 1 | f1 + f2 | X1 ~ Z1 + Z2\", data=data)\niv_fit.summary()\n```\n\nIf the model does not contain any fixed effects, just drop the second part of the formula above:\n\n```{python}\npf.feols(fml=\"Y ~ 1 | X1 ~ Z1 + Z2\", data=data).summary()\n```\n\nYou can access the first stage regression object via the `._model_1st_stage` attribute:\n\n```{python}\npf.etable([iv_fit._model_1st_stage, iv_fit])\n```\n\nYou can access the F-Statistic of the first stage via the `_f_stat_1st_stage` attribute:\n\n```{python}\niv_fit._f_stat_1st_stage\n```\n\nVia the `IV_Diag` method, you can compute additional IV Diagnostics, as the **effective F-statistic** following Olea & Pflueger (2013):\n\n```{python}\niv_fit.IV_Diag()\niv_fit._eff_F\n```\n\nIV estimation with multiple endogenous variables and multiple estimation syntax is currently not supported.\n\n# Poisson Regression\n\nIt is possible to estimate Poisson Regressions (for example, to model count data). We can showcase this feature with another synthetic data set.\n\n\n```{python}\npois_data = pf.get_data(model=\"Fepois\")\n\nax = pois_data.plot(\n    kind=\"scatter\",\n    x=\"X1\",\n    y=\"Y\",\n    c=\"group_id\",\n    colormap=\"viridis\",\n    s=\"f2\",\n)\n```\n\n```{python}\npois_fit = pf.fepois(fml=\"Y ~ X1 | group_id\", data=pois_data, vcov={\"CRV1\": \"group_id\"})\npois_fit.summary()\n```\n\n# Tests of Multiple Hypothesis / Wald Tests\n\nYou can test multiple hypotheses simultaneously via the `wald_test` method.\n\n```{python}\nfit = pf.feols(\"Y ~ X1 + X2 | f1\", data=data)\n```\n\nFor example, to test the joint null hypothesis of $X_{1} = 0$ and $X_{2} = 0$ vs the alternative that $X_{1} \\neq 0$ or $X_{2} \\neq 0$, we would run\n\n```{python}\nfit.wald_test(R=np.eye(2))\n```\n\nAlternatively, suppose we wanted to test a more complicated joint null hypothesis:  $X_{1} + 2X_{2} = 2.0$ and $X_{2} = 1.0$. To do so, we would define $R$ and $q$ as\n\n```{python}\nR1 = np.array([[1, 2], [0, 1]])\nq1 = np.array([2.0, 1.0])\nfit.wald_test(R=R1, q=q1)\n```\n\n# Other GLMs (without fixed effects)\n\n`PyFixest` experimentally supports a range of other GLMs without fixed effects (adding fixed effect support is WIP) via the `pf.feglm()` function. Full support with all bells and whistles (in particular, fixed effects demeaning) is planned for PyFixest 0.29.\n\n```{python}\ndata_glm = pf.get_data(N=100, seed = 170)\ndata_glm[\"Y\"] = np.where(data_glm[\"Y\"] > 0, 1, 0)\n\nfit_gaussian = pf.feglm(fml = \"Y~X1\", data = data_glm, family = \"gaussian\")\nfit_logit = pf.feglm(fml = \"Y~X1\", data = data_glm, family = \"logit\")\nfit_probit = pf.feglm(fml = \"Y~X1\", data = data_glm, family = \"probit\")\n\npf.etable([\n    fit_gaussian,\n    fit_logit,\n    fit_probit,\n])\n```\n\nYou can make predictions on the `response` and `link` scale via the `predict()` method:\n\n```{python}\nfit_logit.predict(type = \"response\")[0:5]\nfit_logit.predict(type = \"link\")[0:5]\n```\n\nYou can compute the **average marginal effect** via the [marginaleffects package](https://github.com/vincentarelbundock/pymarginaleffects):\n\n```{python}\navg_slopes(fit_logit, variables = \"X1\")\n```\n\nPlease take a look at the [marginaleffects book](https://marginaleffects.com/) to learn about other transformations that the `marginaleffects` package supports.\n\n# Multiple Estimation\n\n`PyFixest` supports a range of multiple estimation functionality: `sw`, `sw0`, `csw`, `csw0`, and multiple dependent variables. The meaning of these options is explained in the [Multiple Estimations](https://lrberge.github.io/fixest/articles/multiple_estimations.html) vignette of the `fixest` package:\n\n> - `sw`: this function is replaced sequentially by each of its arguments. For example, `y ~ x1 + sw(x2, x3)` leads to two estimations: `y ~ x1 + x2` and `y ~ x1 + x3`.\n> - `sw0`: identical to sw but first adds the empty element. E.g. `y ~ x1 + sw0(x2, x3)` leads to three estimations: `y ~ x1`, `y ~ x1 + x2` and `y ~ x1 + x3`.\n> - `csw`: it stands for cumulative stepwise. It adds to the formula each of its arguments sequentially. E.g. `y ~ x1 + csw(x2, x3)` will become `y ~ x1 + x2` and `y ~ x1 + x2 + x3`.\n> - `csw0`: identical to csw but first adds the empty element. E.g. `y ~ x1 + csw0(x2, x3)` leads to three estimations: `y ~ x1`, `y ~ x1 + x`2 and `y ~ x1 + x2 + x3`.\n\nAdditionally, we support `split` and `fsplit` function arguments.\n> - `split` allows to split a sample by a given variable. If specified, `pf.feols()` and `pf.fepois()` will loop through all resulting sample splits.\n> - `fsplit` works just as `split`, but fits the model on the full sample as well.\n\nIf multiple regression syntax is used,\n`feols()` and `fepois` returns an instance of a `FixestMulti` object, which essentially consists of a dicionary of `Fepois` or [Feols](/reference/Feols.qmd) instances.\n\n```{python}\nmulti_fit = pf.feols(fml=\"Y ~ X1 | csw0(f1, f2)\", data=data, vcov=\"HC1\")\nmulti_fit\n```\n\n```{python}\nmulti_fit.etable()\n```\n\nYou can access an individual model by its name - i.e. a formula - via the `all_fitted_models` attribute.\n\n```{python}\nmulti_fit.all_fitted_models[\"Y~X1\"].tidy()\n```\n\nor equivalently via the `fetch_model` method:\n\n```{python}\nmulti_fit.fetch_model(0).tidy()\n```\n\nHere, `0` simply fetches the first model stored in the `all_fitted_models` dictionary, `1` the second etc.\n\nObjects of type `Fixest` come with a range of additional methods: `tidy()`, `coef()`, `vcov()` etc, which\nessentially loop over the equivalent methods of all fitted models. E.g. `Fixest.vcov()` updates inference for all\nmodels stored in `Fixest`.\n\n\n```{python}\nmulti_fit.vcov(\"iid\").summary()\n```\n\nYou can summarize multiple models at once via `etable()`. `etable()` has many options to customize the output to obtain publication-ready tables.\n\n```{python}\npf.etable(\n    [fit, fit2],\n    labels={\"Y\": \"Wage\", \"X1\": \"Age\", \"X2\": \"Years of Schooling\"},\n    felabels={\"f1\": \"Industry Fixed Effects\"},\n    caption=\"Regression Results\",\n)\n```\n\nYou can also visualize multiple estimation results via `iplot()` and `coefplot()`:\n\n```{python}\nmulti_fit.coefplot().show()\n```\n\n# Difference-in-Differences / Event Study Designs\n\n`PyFixest` supports eventy study designs via two-way fixed effects, Gardner's 2-stage estimator, and the linear projections estimator.\n\n```{python}\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\n\ndf_het.head()\n```\n\n\n```{python}\nfit_did2s = pf.did2s(\n    df_het,\n    yname=\"dep_var\",\n    first_stage=\"~ 0 | state + year\",\n    second_stage=\"~i(rel_year,ref= -1.0)\",\n    treatment=\"treat\",\n    cluster=\"state\",\n)\n\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(rel_year,ref = -1.0) | state + year\",\n    df_het,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfrom pyfixest.report.utils import rename_categoricals\npf.iplot(\n    [fit_did2s, fit_twfe], coord_flip=False, figsize=(900, 400), title=\"TWFE vs DID2S\", rotate_xticks=90,\n    labels= rename_categoricals(fit_did2s._coefnames, template=\"{value_int}\")\n)\n```\n\n\nThe `event_study()` function provides a common API for several event study estimators.\n\n```{python}\nfit_twfe = pf.event_study(\n    data=df_het,\n    yname=\"dep_var\",\n    idname=\"state\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"twfe\",\n)\n\nfit_did2s = pf.event_study(\n    data=df_het,\n    yname=\"dep_var\",\n    idname=\"state\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"did2s\",\n)\n\npf.etable([fit_twfe, fit_did2s])\n```\n\nFor more details see the vignette on [Difference-in-Differences Estimation](https://py-econometrics.github.io/pyfixest/difference-in-differences.html).\n","srcMarkdownNoYaml":"\n\n# OLS with Fixed Effects\n\n## What is a fixed effect model?\n\nA fixed effect model is a statistical model that includes fixed effects, which are parameters that are estimated to be constant across different groups.\n\n**Example [Panel Data]:** In the context of panel data, fixed effects are parameters that are constant across different individuals or time. The typical model example is given by the following equation:\n\n$$\nY_{it} = \\beta X_{it} + \\alpha_i + \\psi_t + \\varepsilon_{it}\n$$\n\nwhere $Y_{it}$ is the dependent variable for individual $i$ at time $t$, $X_{it}$ is the independent variable, $\\beta$ is the coefficient of the independent variable, $\\alpha_i$ is the individual fixed effect, $\\psi_t$ is the time fixed effect, and $\\varepsilon_{it}$ is the error term. The individual fixed effect $\\alpha_i$ is a parameter that is constant across time for each individual, while the time fixed effect $\\psi_t$ is a parameter that is constant across individuals for each time period.\n\nNote however that, despite the fact that fixed effects are commonly used in panel setting, one does not need a panel data set to work with fixed effects. For example, cluster randomized trials with cluster fixed effects, or wage regressions with worker and firm fixed effects.\n\nIn this \"quick start\" guide, we will show you how to estimate a fixed effect model using the `PyFixest` package. We do not go into the details of the theory behind fixed effect models, but we focus on how to estimate them using `PyFixest`.\n\n## Read Sample Data\n\nIn a first step, we load the module and some synthetic example data:\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\ntry:\n    from lets_plot import LetsPlot\n    _HAS_LETS_PLOT = True\nexcept ImportError:\n    _HAS_LETS_PLOT = False\n\nfrom marginaleffects import slopes, avg_slopes\n\nimport pyfixest as pf\n\nif _HAS_LETS_PLOT:\n    LetsPlot.setup_html()\n\nplt.style.use(\"seaborn-v0_8\")\n\n%load_ext watermark\n%config InlineBackend.figure_format = \"retina\"\n%watermark --iversions\n\ndata = pf.get_data()\n\ndata.head()\n```\n\n```{python}\ndata.info()\n```\n\nWe see that some of our columns have missing data.\n\n## OLS Estimation\n\nWe are interested in the relation between the dependent variable `Y` and the independent variables `X1` using a fixed effect model for `group_id`. Let's see how the data looks like:\n\n```{python}\nax = data.plot(kind=\"scatter\", x=\"X1\", y=\"Y\", c=\"group_id\", colormap=\"viridis\")\n```\n\nWe can estimate a fixed effects regression via the `feols()` function. `feols()` has three arguments: a two-sided model formula, the data, and optionally, the type of inference.\n\n```{python}\nfit = pf.feols(fml=\"Y ~ X1 | group_id\", data=data, vcov=\"HC1\")\ntype(fit)\n```\n\n\nThe first part of the formula contains the dependent variable and \"regular\" covariates, while the second part contains fixed effects.\n\n`feols()` returns an instance of the `Fixest` class.\n\n## Inspecting Model Results\n\nTo inspect the results, we can use a summary function or method:\n\n```{python}\nfit.summary()\n```\n\nOr display a formatted regression table:\n\n```{python}\npf.etable(fit)\n```\n\nAlternatively, the `.summarize` module contains a `summary` function, which can be applied on instances of regression model objects or lists of regression model objects. For details on how to customize `etable()`, please take a look at the [dedicated vignette](https://py-econometrics.github.io/pyfixest/table-layout.html).\n\n```{python}\npf.summary(fit)\n```\n\nYou can access individual elements of the summary via dedicated methods: `.tidy()` returns a \"tidy\" `pd.DataFrame`,\n`.coef()` returns estimated parameters, and `se()` estimated standard errors. Other methods include `pvalue()`, `confint()`\nand `tstat()`.\n\n```{python}\nfit.tidy()\n```\n\n```{python}\nfit.coef()\n```\n\n```{python}\nfit.se()\n```\n\n```{python}\nfit.tstat()\n```\n\n```{python}\nfit.confint()\n```\n\nLast, model results can be visualized via dedicated methods for plotting:\n\n```{python}\nfit.coefplot()\n# or pf.coefplot([fit])\n```\n\n## How to interpret the results?\n\nLet's have a quick d-tour on the intuition behind fixed effects models using the example above. To do so, let us begin by comparing it with a simple OLS model.\n\n```{python}\nfit_simple = pf.feols(\"Y ~ X1\", data=data, vcov=\"HC1\")\n\nfit_simple.summary()\n```\n\nWe can compare both models side by side in a regression table:\n\n```{python}\npf.etable([fit, fit_simple])\n```\n\nWe see that the `X1` coefficient is `-1.019`, which is less than the value from the OLS model in column (2). Where is the difference coming from?\nWell, in the fixed effect model we are interested in controlling for the feature `group_id`. One possibility to do this is by adding a simple dummy variable for each level of `group_id`.\n\n```{python}\nfit_dummy = pf.feols(\"Y ~ X1 + C(group_id) \", data=data, vcov=\"HC1\")\n\nfit_dummy.summary()\n```\n\nThis is does not scale well! Imagine you have 1000 different levels of `group_id`. You would need to add 1000 dummy variables to your model. This is where fixed effect models come in handy. They allow you to control for these fixed effects without adding all these dummy variables. The way to do it is by a *demeaning procedure*. The idea is to subtract the average value of each level of `group_id` from the respective observations. This way, we control for the fixed effects without adding all these dummy variables. Let's try to do this manually:\n\n```{python}\ndef _demean_column(df: pd.DataFrame, column: str, by: str) -> pd.Series:\n    return df[column] - df.groupby(by)[column].transform(\"mean\")\n\n\nfit_demeaned = pf.feols(\n    fml=\"Y_demeaned ~ X1_demeaned\",\n    data=data.assign(\n        Y_demeaned=lambda df: _demean_column(df, \"Y\", \"group_id\"),\n        X1_demeaned=lambda df: _demean_column(df, \"X1\", \"group_id\"),\n    ),\n    vcov=\"HC1\",\n)\n\nfit_demeaned.summary()\n```\n\nWe get the same results as the fixed effect model `Y1 ~ X | group_id` above. The `PyFixest` package uses a more efficient algorithm to estimate the fixed effect model, but the intuition is the same.\n\n## Updating Regression Coefficients\n\nYou can update the coefficients of a model object via the `update()` method, which may be useful in an online learning setting where data arrives sequentially.\n\nTo see this in action, let us first fit a model on a subset of the data:\n\n```{python}\ndata_subsample = data.sample(frac=0.5)\nm = pf.feols(\"Y ~ X1 + X2\", data=data_subsample)\n# current coefficient vector\nm._beta_hat\n```\n\nThen sample 5 new observations and update the model with the new data. The update rule is\n\n$$\n\\hat{\\beta}_{n+1} = \\hat{\\beta}_n + (X_{n+1}' X_{n+1})^{-1} x_{n+1} + (y_{n+1} - x_{n+1} \\hat{\\beta}_n)\n$$\n\nfor a new observation $(x_{n+1}, y_{n+1})$.\n\n```{python}\nnew_points_id = np.random.choice(list(set(data.index) - set(data_subsample.index)), 5)\nX_new, y_new = (\n    np.c_[np.ones(len(new_points_id)), data.loc[new_points_id][[\"X1\", \"X2\"]].values],\n    data.loc[new_points_id][\"Y\"].values,\n)\nm.update(X_new, y_new)\n```\n\nWe verify that we get the same results if we had estimated the model on the appended data.\n\n```{python}\npf.feols(\n    \"Y ~ X1 + X2\", data=data.loc[data_subsample.index.append(pd.Index(new_points_id))]\n).coef().values\n```\n\n# Standard Errors and Inference\n\nSupported covariance types are \"iid\", \"HC1-3\", CRV1 and CRV3 (up to two-way clustering).\n\n**Why do we have so many different types of standard errors?**\n\nThe standard errors of the coefficients are crucial for inference. They tell us how certain we can be about the estimated coefficients. In the presence of heteroskedasticity (a situation which typically arises with cross-sectional data), the standard OLS standard errors are biased. The `pyfixest` package provides several types of standard errors that are robust to heteroskedasticity.\n\n- `iid`: assumes that the error variance is spherical, i.e. errors are homoskedastic and not correlated (independent and identically distributed errors have a spherical error variance).\n- `HC1-3`: heteroskedasticity-robust standard errors according to White (1980) and MacKinnon and White (1985). See [Econometric Computing with HC and HAC\nCovariance Matrix Estimators](https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich.pdf) from the [`sandwich`](https://cran.r-project.org/web/packages/sandwich/) package for more details.\n- `CRV1` and `CRV3`: cluster robust standard errors according to Cameron, Gelbach, and Miller (2011). See [A Practitioner's Guide to Cluster-Robust Inference](https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf).  For   `CRV1` and `CRV3` one should pass a dictionaty of the form `{\"CRV1\": \"clustervar\"}`.\n\nInference can be adjusted \"on-the-fly\" via the `.vcov()` method:\n\n```{python}\nfit.vcov({\"CRV1\": \"group_id + f2\"}).summary()\n\nfit.vcov({\"CRV3\": \"group_id\"}).summary()\n```\n\nThe estimated covariance matrix is available as an attribute of the `Feols` object called `._vcov`.\n\n## Inference via the Wild Bootstrap\n\nIt is also possible to run a wild (cluster) bootstrap after estimation (via the [wildboottest module](https://github.com/py-econometrics/wildboottest), see [MacKinnon, J. G., Nielsen, M. Ø., & Webb, M. D. (2023). Fast and reliable jackknife and bootstrap methods for cluster-robust inference. Journal of Applied Econometrics, 38(5), 671–694.](http://qed.econ.queensu.ca/pub/faculty/mackinnon/working-papers/qed_wp_1485.pdf)):\n\n```{python}\nfit2 = pf.feols(fml=\"Y ~ X1\", data=data, vcov={\"CRV1\": \"group_id\"})\nfit2.wildboottest(param=\"X1\", reps=999)\n```\n\n## The Causal Cluster Variance Estimator\n\nAdditionally, `PyFixest` supports the causal cluster variance estimator following [Abadie et al. (2023)](https://academic.oup.com/qje/article/138/1/1/6750017). Let's look into it with another data set:\n\n```{python}\ndf = pd.read_stata(\"http://www.damianclarke.net/stata/census2000_5pc.dta\")\n\ndf.head()\n```\n\n\n```{python}\naxes = df.plot.hist(column=[\"ln_earnings\"], by=[\"college\"])\n```\n\nNow we can estimate the model `ln_earnings ~ college` where we cluster the standard errors at the state level:\n\n```{python}\nfit3 = pf.feols(\"ln_earnings ~ college\", vcov={\"CRV1\": \"state\"}, data=df)\nfit3.ccv(treatment=\"college\", pk=0.05, n_splits=2, seed=929)\n```\n\n## Randomization Inference\n\nYou can also conduct inference via randomization inference [(see Heß, Stata Journal 2017)](https://hesss.org/ritest.pdf).\n`PyFixest` supports random and cluster random sampling.\n\n```{python}\nfit2.ritest(resampvar=\"X1=0\", reps=1000, cluster=\"group_id\")\n```\n\n## Multiple Testing Corrections: Bonferroni and Romano-Wolf\n\nTo correct for multiple testing, p-values can be adjusted via either the [Bonferroni](https://en.wikipedia.org/wiki/Bonferroni_correction), the method by Romano and Wolf (2005), see for example [The Romano-Wolf Multiple Hypothesis\nCorrection in Stata](https://docs.iza.org/dp12845.pdf), and the method by Westfall & Young (see [here](https://www.jstor.org/stable/2532216)).\n\n```{python}\npf.bonferroni([fit, fit2], param=\"X1\").round(3)\n```\n\n```{python}\npf.rwolf([fit, fit2], param=\"X1\", reps=9999, seed=1234).round(3)\n```\n\n```{python}\npf.wyoung([fit, fit2], param=\"X1\", reps=9999, seed=1234).round(3)\n```\n\n## Joint Confidence Intervals\n\nSimultaneous confidence bands for a vector of parameters can be computed via the `joint_confint()` method. See [Simultaneous confidence bands: Theory, implementation, and an application to SVARs](https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2656) for background.\n\n```{python}\nfit_ci = pf.feols(\"Y ~ X1+ C(f1)\", data=data)\nfit_ci.confint(joint=True).head()\n```\n\n# Panel Data Example: Causal Inference for the Brave and True\n\nIn this example we replicate the results of the great (freely available reference!) [Causal Inference for the Brave and True - Chapter 14](https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-Fixed-Effects.html). Please refer to the original text for a detailed explanation of the data.\n\n```{python}\ndata_path = \"https://raw.githubusercontent.com/bashtage/linearmodels/main/linearmodels/datasets/wage_panel/wage_panel.csv.bz2\"\ndata_df = pd.read_csv(data_path)\n\ndata_df.head()\n```\n\nThe objective is to estimate the effect of the variable `married` on the variable `lwage` using a fixed effect model on the entity variable `nr` and the time variable `year`.\n\n```{python}\npanel_fit = pf.feols(\n    fml=\"lwage ~ expersq + union + married + hours | nr + year\",\n    data=data_df,\n    vcov={\"CRV1\": \"nr + year\"},\n)\n\npf.etable(panel_fit)\n```\n\nWe obtain the same results as in the book!\n\n# Instrumental Variables (IV) Estimation\n\nIt is also possible to estimate [instrumental variable models](https://en.wikipedia.org/wiki/Instrumental_variables_estimation) with *one* endogenous variable and (potentially multiple) instruments.\n\nIn general, the syntax for IV is `depvar ~ exog.vars | fixef effects | endog.vars ~ instruments`.\n\n```{python}\niv_fit = pf.feols(fml=\"Y2 ~ 1 | f1 + f2 | X1 ~ Z1 + Z2\", data=data)\niv_fit.summary()\n```\n\nIf the model does not contain any fixed effects, just drop the second part of the formula above:\n\n```{python}\npf.feols(fml=\"Y ~ 1 | X1 ~ Z1 + Z2\", data=data).summary()\n```\n\nYou can access the first stage regression object via the `._model_1st_stage` attribute:\n\n```{python}\npf.etable([iv_fit._model_1st_stage, iv_fit])\n```\n\nYou can access the F-Statistic of the first stage via the `_f_stat_1st_stage` attribute:\n\n```{python}\niv_fit._f_stat_1st_stage\n```\n\nVia the `IV_Diag` method, you can compute additional IV Diagnostics, as the **effective F-statistic** following Olea & Pflueger (2013):\n\n```{python}\niv_fit.IV_Diag()\niv_fit._eff_F\n```\n\nIV estimation with multiple endogenous variables and multiple estimation syntax is currently not supported.\n\n# Poisson Regression\n\nIt is possible to estimate Poisson Regressions (for example, to model count data). We can showcase this feature with another synthetic data set.\n\n\n```{python}\npois_data = pf.get_data(model=\"Fepois\")\n\nax = pois_data.plot(\n    kind=\"scatter\",\n    x=\"X1\",\n    y=\"Y\",\n    c=\"group_id\",\n    colormap=\"viridis\",\n    s=\"f2\",\n)\n```\n\n```{python}\npois_fit = pf.fepois(fml=\"Y ~ X1 | group_id\", data=pois_data, vcov={\"CRV1\": \"group_id\"})\npois_fit.summary()\n```\n\n# Tests of Multiple Hypothesis / Wald Tests\n\nYou can test multiple hypotheses simultaneously via the `wald_test` method.\n\n```{python}\nfit = pf.feols(\"Y ~ X1 + X2 | f1\", data=data)\n```\n\nFor example, to test the joint null hypothesis of $X_{1} = 0$ and $X_{2} = 0$ vs the alternative that $X_{1} \\neq 0$ or $X_{2} \\neq 0$, we would run\n\n```{python}\nfit.wald_test(R=np.eye(2))\n```\n\nAlternatively, suppose we wanted to test a more complicated joint null hypothesis:  $X_{1} + 2X_{2} = 2.0$ and $X_{2} = 1.0$. To do so, we would define $R$ and $q$ as\n\n```{python}\nR1 = np.array([[1, 2], [0, 1]])\nq1 = np.array([2.0, 1.0])\nfit.wald_test(R=R1, q=q1)\n```\n\n# Other GLMs (without fixed effects)\n\n`PyFixest` experimentally supports a range of other GLMs without fixed effects (adding fixed effect support is WIP) via the `pf.feglm()` function. Full support with all bells and whistles (in particular, fixed effects demeaning) is planned for PyFixest 0.29.\n\n```{python}\ndata_glm = pf.get_data(N=100, seed = 170)\ndata_glm[\"Y\"] = np.where(data_glm[\"Y\"] > 0, 1, 0)\n\nfit_gaussian = pf.feglm(fml = \"Y~X1\", data = data_glm, family = \"gaussian\")\nfit_logit = pf.feglm(fml = \"Y~X1\", data = data_glm, family = \"logit\")\nfit_probit = pf.feglm(fml = \"Y~X1\", data = data_glm, family = \"probit\")\n\npf.etable([\n    fit_gaussian,\n    fit_logit,\n    fit_probit,\n])\n```\n\nYou can make predictions on the `response` and `link` scale via the `predict()` method:\n\n```{python}\nfit_logit.predict(type = \"response\")[0:5]\nfit_logit.predict(type = \"link\")[0:5]\n```\n\nYou can compute the **average marginal effect** via the [marginaleffects package](https://github.com/vincentarelbundock/pymarginaleffects):\n\n```{python}\navg_slopes(fit_logit, variables = \"X1\")\n```\n\nPlease take a look at the [marginaleffects book](https://marginaleffects.com/) to learn about other transformations that the `marginaleffects` package supports.\n\n# Multiple Estimation\n\n`PyFixest` supports a range of multiple estimation functionality: `sw`, `sw0`, `csw`, `csw0`, and multiple dependent variables. The meaning of these options is explained in the [Multiple Estimations](https://lrberge.github.io/fixest/articles/multiple_estimations.html) vignette of the `fixest` package:\n\n> - `sw`: this function is replaced sequentially by each of its arguments. For example, `y ~ x1 + sw(x2, x3)` leads to two estimations: `y ~ x1 + x2` and `y ~ x1 + x3`.\n> - `sw0`: identical to sw but first adds the empty element. E.g. `y ~ x1 + sw0(x2, x3)` leads to three estimations: `y ~ x1`, `y ~ x1 + x2` and `y ~ x1 + x3`.\n> - `csw`: it stands for cumulative stepwise. It adds to the formula each of its arguments sequentially. E.g. `y ~ x1 + csw(x2, x3)` will become `y ~ x1 + x2` and `y ~ x1 + x2 + x3`.\n> - `csw0`: identical to csw but first adds the empty element. E.g. `y ~ x1 + csw0(x2, x3)` leads to three estimations: `y ~ x1`, `y ~ x1 + x`2 and `y ~ x1 + x2 + x3`.\n\nAdditionally, we support `split` and `fsplit` function arguments.\n> - `split` allows to split a sample by a given variable. If specified, `pf.feols()` and `pf.fepois()` will loop through all resulting sample splits.\n> - `fsplit` works just as `split`, but fits the model on the full sample as well.\n\nIf multiple regression syntax is used,\n`feols()` and `fepois` returns an instance of a `FixestMulti` object, which essentially consists of a dicionary of `Fepois` or [Feols](/reference/Feols.qmd) instances.\n\n```{python}\nmulti_fit = pf.feols(fml=\"Y ~ X1 | csw0(f1, f2)\", data=data, vcov=\"HC1\")\nmulti_fit\n```\n\n```{python}\nmulti_fit.etable()\n```\n\nYou can access an individual model by its name - i.e. a formula - via the `all_fitted_models` attribute.\n\n```{python}\nmulti_fit.all_fitted_models[\"Y~X1\"].tidy()\n```\n\nor equivalently via the `fetch_model` method:\n\n```{python}\nmulti_fit.fetch_model(0).tidy()\n```\n\nHere, `0` simply fetches the first model stored in the `all_fitted_models` dictionary, `1` the second etc.\n\nObjects of type `Fixest` come with a range of additional methods: `tidy()`, `coef()`, `vcov()` etc, which\nessentially loop over the equivalent methods of all fitted models. E.g. `Fixest.vcov()` updates inference for all\nmodels stored in `Fixest`.\n\n\n```{python}\nmulti_fit.vcov(\"iid\").summary()\n```\n\nYou can summarize multiple models at once via `etable()`. `etable()` has many options to customize the output to obtain publication-ready tables.\n\n```{python}\npf.etable(\n    [fit, fit2],\n    labels={\"Y\": \"Wage\", \"X1\": \"Age\", \"X2\": \"Years of Schooling\"},\n    felabels={\"f1\": \"Industry Fixed Effects\"},\n    caption=\"Regression Results\",\n)\n```\n\nYou can also visualize multiple estimation results via `iplot()` and `coefplot()`:\n\n```{python}\nmulti_fit.coefplot().show()\n```\n\n# Difference-in-Differences / Event Study Designs\n\n`PyFixest` supports eventy study designs via two-way fixed effects, Gardner's 2-stage estimator, and the linear projections estimator.\n\n```{python}\nurl = \"https://raw.githubusercontent.com/py-econometrics/pyfixest/master/pyfixest/did/data/df_het.csv\"\ndf_het = pd.read_csv(url)\n\ndf_het.head()\n```\n\n\n```{python}\nfit_did2s = pf.did2s(\n    df_het,\n    yname=\"dep_var\",\n    first_stage=\"~ 0 | state + year\",\n    second_stage=\"~i(rel_year,ref= -1.0)\",\n    treatment=\"treat\",\n    cluster=\"state\",\n)\n\n\nfit_twfe = pf.feols(\n    \"dep_var ~ i(rel_year,ref = -1.0) | state + year\",\n    df_het,\n    vcov={\"CRV1\": \"state\"},\n)\n\nfrom pyfixest.report.utils import rename_categoricals\npf.iplot(\n    [fit_did2s, fit_twfe], coord_flip=False, figsize=(900, 400), title=\"TWFE vs DID2S\", rotate_xticks=90,\n    labels= rename_categoricals(fit_did2s._coefnames, template=\"{value_int}\")\n)\n```\n\n\nThe `event_study()` function provides a common API for several event study estimators.\n\n```{python}\nfit_twfe = pf.event_study(\n    data=df_het,\n    yname=\"dep_var\",\n    idname=\"state\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"twfe\",\n)\n\nfit_did2s = pf.event_study(\n    data=df_het,\n    yname=\"dep_var\",\n    idname=\"state\",\n    tname=\"year\",\n    gname=\"g\",\n    estimator=\"did2s\",\n)\n\npf.etable([fit_twfe, fit_did2s])\n```\n\nFor more details see the vignette on [Difference-in-Differences Estimation](https://py-econometrics.github.io/pyfixest/difference-in-differences.html).\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"python":"/pyfixest/.pixi/envs/docs/Scripts/python.exe","engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"html-table-processing":"none"},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"quickstart.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","quartodoc":{"package":"pyfixest","title":"PyFixest Function Reference","parser":"numpy","rewrite_all_pages":false,"sidebar":"_sidebar.yml","sections":[{"title":"Estimation Functions","desc":"User facing estimation functions\n","contents":["estimation.estimation.feols","estimation.estimation.fepois","estimation.estimation.feglm","did.estimation.did2s","did.estimation.lpdid","did.estimation.event_study","estimation.bonferroni","estimation.rwolf"]},{"title":"Estimation Classes","desc":"Details on Methods and Attributes\n","contents":["estimation.feols_.Feols","estimation.fepois_.Fepois","estimation.feiv_.Feiv","estimation.feglm_.Feglm","estimation.felogit_.Felogit","estimation.feprobit_.Feprobit","estimation.fegaussian_.Fegaussian","estimation.feols_compressed_.FeolsCompressed"]},{"title":"Summarize and Visualize","desc":"Post-Processing of Estimation Results\n","contents":["did.visualize.panelview","report.summary","report.etable","report.dtable","report.coefplot","report.iplot","did.visualize.panelview"]},{"title":"Misc / Utilities","desc":"PyFixest internals and utilities\n","contents":["estimation.demean","estimation.detect_singletons","estimation.model_matrix_fixest"]}]},"title":"Getting Started with PyFixest","toc-title":"On this page","toc-location":"left"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
{"title":"How much do Covariates Matter?","markdown":{"yaml":{"title":"How much do Covariates Matter?"},"headingText":"Motivation","containsRefs":false,"markdown":"\n\n\n\n\n\n\n\nIn regression analyses, we often wonder about \"how much covariates matter?\" for explaining the relationship between a target variable $D$ and an outcome variable $Y$. \n\nFor example, we might start analysing the gender wage gap with a simple regression model as `log(wage) on gender`. But arguably, men and women differ in many socio-economic characteristics: they might have different (average) levels of education or career experience, and they might work in different industries and select into different higher- or lower-paying industries. So which fraction of the gender wage gap can be explained by these observable characteristics? \n\nIn this notebook, we will compute and decompose the gender wage gab based on a subset of the PSID data set using a method commonly known as the \n\"Gelbach Decomposition\" ([Gelbach, JoLE 2016](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)). \n\nWe start with loading a subset of the PSID data provided by the AER R package. \n\nComputing a first correlation between gender and wage, we find that males earn on average 0.474 log points\nmore than women. \n\nTo examine the impact of observable on the relationship between wage and gender, a common strategy in applied research is to incrementally add a set of covariates to the baseline regression model of `log(wage) on gender`. Here, we will incrementally add the following covariates:  \n\n- education,\n- experience\n- occupation, \n- industry \n- year \n- ethnicity\n\nWe can do so by using **multiple estimation syntax**: \n\nBecause the table is so long that it's hard to see anything, we restrict it to display only a few variables: \n\nWe see that the coefficient on gender is roughly the same in all models. Tentatively, we might already conclude that the observable characteristics in the data do not explain a large part of the gender wage gap. \n\nBut how much do differences in education matter? We have computed 6 additional models that contain education as a covariate. The obtained point estimates \nvary between $0.059$ and $0.075$. Which of these numbers should we report?  \n\nAdditionally, note that while we have only computed 6 additional models with covariates, the number of possible models is much larger. \nIf I did the math correctly, simply by additively and incrementally adding covariates, we could have computed $57$ different models (not all of which would have included `education` as a control).\n\nAs it turns out, different models **will lead to different point estimates**. The order of incrementally adding covariates **might** impact our conclusion. To illustrate this, we keep the same ordering as before, but start with `ethnicity` as our first variable: \n\nWe obtain 5 new coefficients on `education` that vary between 0.074 and 0.059. \n\nSo, which share of the \"raw\" gender wage gap can be attributed to differences in education between men and women? Should we report a statisics based on the 0.075 estimate? Or \non the 0.059 estimate? Which value should we pick? \n\nTo help us with this problem, Gelbach (2016, JoLE) develops a decomposition procedure building on the omitted variable bias formula that produces a single value for the contribution of a given covariate, say education, to the gender wage gap.\n\n## Notation and Gelbach's Algorithm\n\nBefore we dive into a code example, let us first introduce the notation and Gelbach's algorithm. We are interested in \"decomposing\" the effect of \na variable $X_{1} \\in \\mathbb{R}$ on an outcome $Y \\in \\mathbb{R}$ into a part explained by covariates $X_{2} \\in \\mathbb{R}^{k_{2}}$ and an unexplained part. \n\nThus we can specify two regression models: \n\n- The **short** model \n    $$\n        Y = X_{1} \\beta_{1} + u_{1}\n    $$\n\n- the **long** (or full) model \n\n    $$\n        Y = X_{1} \\beta_{1} + X_{2} \\beta_{2} + e\n    $$\n\nBy fitting the **short** regression, we obtain an estimate $\\hat{\\beta}_{1}$, which we will denote as the **direct effect**, and by estimating the **long** regression, we obtain an estimate of the regression coefficients $\\hat{\\beta}_{2} \\in \\mathbb{R}^{k_2}$. We will denote the estimate on $X_1$ in the long regression as the **full** effect. \n\nWe can then compute the contribution of an individual covariate $\\hat{\\delta}_{k}$ via the following algorithm: \n\n- Step 1: we compute coefficients from $k_{2}$ auxiliary regression models $\\hat{\\Gamma}$ as \n    $$\n        \\hat{\\Gamma} = (X_{1}'X_{1})^{-1} X_{1}'X_{2}\n    $$\n\n    In words, we regress the target variable $X_{1}$ on each covariate in $X_{2}$. In practice, we can easily do this in one line of code via `scipy.linalg.lstsq()`.\n\n- Step 2: We can compute the total effect **explained** by the covariates, which we denote by $\\delta$, as \n\n    $$\n        \\hat{\\delta} = \\sum_{k=1}^{k_2} \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}\n    $$\n\n    where $\\hat{\\Gamma}_{k}$ are the coefficients from an auxiliary regression $X_1$ on covariate $X_{2,k}$ and $\\hat{\\beta}_{2,k}$ is the associated estimate on $X_{2,k}$ from the **full** model. \n\n    The individual **contribution of covariate $k$** is then defined as \n\n    $$\n        \\hat{\\delta}_{k} = \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}.\n    $$\n\n\nAfter having obtained $\\delta_{k}$ for each auxiliary variable $k$, we can easily aggregate multiple variables into a single groups of interest. For example, if $X_{2}$ contains a set of dummies from industry fixed effects, we could compute the explained part of \"industry\" by summing over all the dummies: \n\n$$\n        \\hat{\\delta}_{\\textit{industry}} = \\sum_{k \\in \\textit{industry dummies}} \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}\n$$\n\n## `PyFixest` Example\n\nTo employ Gelbach's decomposition in `pyfixest`, we start with the **full** regression model that contains **all variables of interest**: \n\nAfter fitting the **full model**, we can run the decomposition procedure by calling the `decompose()` method. The only required argument is to specify the target parameter, which in this case is \"gender\". Inference is conducted via a non-parametric bootstrap and can optionally be turned off.\n\nAs before, this produces a pretty big output table that reports \n- the **direct effect** of the regression of `log(wage) ~ gender`\n- the **full effect** of gender on log wage using the **full regression** with all control variables\n- the **explained effect** as the difference between the full and direct effect\n- a **single scalar value** for the individual contributions of a covariate to overall **explained effect** \n\nFor our example at hand, the additional covariates only explain a tiny fraction of the differences in log wages between men and women - 0.064 points. \nOf these, around one third can be attributed to ethnicity, 0.00064 to years of eduaction, etc. \n\nBecause experience is a categorical variable, the table gets pretty unhandy: we produce one estimate for \"each\" level. Luckily, Gelbach's decomposition \nallows us to group individual contributions into a single number. In the `decompose()` method, we can combine variables via the `combine_covariates` argument: \n\nWe now report a single value for \"experience\", which explains a good chunk - around half - of the explained part of the gender wage gap. \n\nWe can aggregate even more to \"individual level\" and \"job\" level variables: \n\n## Literature \n\n- [\"When do Covariates Matter? And Which Ones, and How Much?\" by Gelbach, Jonah B. (2016), Journal of Labor Economics](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)\n\n\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\n## Motivation\n\nIn regression analyses, we often wonder about \"how much covariates matter?\" for explaining the relationship between a target variable $D$ and an outcome variable $Y$. \n\nFor example, we might start analysing the gender wage gap with a simple regression model as `log(wage) on gender`. But arguably, men and women differ in many socio-economic characteristics: they might have different (average) levels of education or career experience, and they might work in different industries and select into different higher- or lower-paying industries. So which fraction of the gender wage gap can be explained by these observable characteristics? \n\nIn this notebook, we will compute and decompose the gender wage gab based on a subset of the PSID data set using a method commonly known as the \n\"Gelbach Decomposition\" ([Gelbach, JoLE 2016](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)). \n\nWe start with loading a subset of the PSID data provided by the AER R package. \n\nComputing a first correlation between gender and wage, we find that males earn on average 0.474 log points\nmore than women. \n\nTo examine the impact of observable on the relationship between wage and gender, a common strategy in applied research is to incrementally add a set of covariates to the baseline regression model of `log(wage) on gender`. Here, we will incrementally add the following covariates:  \n\n- education,\n- experience\n- occupation, \n- industry \n- year \n- ethnicity\n\nWe can do so by using **multiple estimation syntax**: \n\nBecause the table is so long that it's hard to see anything, we restrict it to display only a few variables: \n\nWe see that the coefficient on gender is roughly the same in all models. Tentatively, we might already conclude that the observable characteristics in the data do not explain a large part of the gender wage gap. \n\nBut how much do differences in education matter? We have computed 6 additional models that contain education as a covariate. The obtained point estimates \nvary between $0.059$ and $0.075$. Which of these numbers should we report?  \n\nAdditionally, note that while we have only computed 6 additional models with covariates, the number of possible models is much larger. \nIf I did the math correctly, simply by additively and incrementally adding covariates, we could have computed $57$ different models (not all of which would have included `education` as a control).\n\nAs it turns out, different models **will lead to different point estimates**. The order of incrementally adding covariates **might** impact our conclusion. To illustrate this, we keep the same ordering as before, but start with `ethnicity` as our first variable: \n\nWe obtain 5 new coefficients on `education` that vary between 0.074 and 0.059. \n\nSo, which share of the \"raw\" gender wage gap can be attributed to differences in education between men and women? Should we report a statisics based on the 0.075 estimate? Or \non the 0.059 estimate? Which value should we pick? \n\nTo help us with this problem, Gelbach (2016, JoLE) develops a decomposition procedure building on the omitted variable bias formula that produces a single value for the contribution of a given covariate, say education, to the gender wage gap.\n\n## Notation and Gelbach's Algorithm\n\nBefore we dive into a code example, let us first introduce the notation and Gelbach's algorithm. We are interested in \"decomposing\" the effect of \na variable $X_{1} \\in \\mathbb{R}$ on an outcome $Y \\in \\mathbb{R}$ into a part explained by covariates $X_{2} \\in \\mathbb{R}^{k_{2}}$ and an unexplained part. \n\nThus we can specify two regression models: \n\n- The **short** model \n    $$\n        Y = X_{1} \\beta_{1} + u_{1}\n    $$\n\n- the **long** (or full) model \n\n    $$\n        Y = X_{1} \\beta_{1} + X_{2} \\beta_{2} + e\n    $$\n\nBy fitting the **short** regression, we obtain an estimate $\\hat{\\beta}_{1}$, which we will denote as the **direct effect**, and by estimating the **long** regression, we obtain an estimate of the regression coefficients $\\hat{\\beta}_{2} \\in \\mathbb{R}^{k_2}$. We will denote the estimate on $X_1$ in the long regression as the **full** effect. \n\nWe can then compute the contribution of an individual covariate $\\hat{\\delta}_{k}$ via the following algorithm: \n\n- Step 1: we compute coefficients from $k_{2}$ auxiliary regression models $\\hat{\\Gamma}$ as \n    $$\n        \\hat{\\Gamma} = (X_{1}'X_{1})^{-1} X_{1}'X_{2}\n    $$\n\n    In words, we regress the target variable $X_{1}$ on each covariate in $X_{2}$. In practice, we can easily do this in one line of code via `scipy.linalg.lstsq()`.\n\n- Step 2: We can compute the total effect **explained** by the covariates, which we denote by $\\delta$, as \n\n    $$\n        \\hat{\\delta} = \\sum_{k=1}^{k_2} \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}\n    $$\n\n    where $\\hat{\\Gamma}_{k}$ are the coefficients from an auxiliary regression $X_1$ on covariate $X_{2,k}$ and $\\hat{\\beta}_{2,k}$ is the associated estimate on $X_{2,k}$ from the **full** model. \n\n    The individual **contribution of covariate $k$** is then defined as \n\n    $$\n        \\hat{\\delta}_{k} = \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}.\n    $$\n\n\nAfter having obtained $\\delta_{k}$ for each auxiliary variable $k$, we can easily aggregate multiple variables into a single groups of interest. For example, if $X_{2}$ contains a set of dummies from industry fixed effects, we could compute the explained part of \"industry\" by summing over all the dummies: \n\n$$\n        \\hat{\\delta}_{\\textit{industry}} = \\sum_{k \\in \\textit{industry dummies}} \\hat{\\Gamma}_{k} \\hat{\\beta}_{2,k}\n$$\n\n## `PyFixest` Example\n\nTo employ Gelbach's decomposition in `pyfixest`, we start with the **full** regression model that contains **all variables of interest**: \n\nAfter fitting the **full model**, we can run the decomposition procedure by calling the `decompose()` method. The only required argument is to specify the target parameter, which in this case is \"gender\". Inference is conducted via a non-parametric bootstrap and can optionally be turned off.\n\nAs before, this produces a pretty big output table that reports \n- the **direct effect** of the regression of `log(wage) ~ gender`\n- the **full effect** of gender on log wage using the **full regression** with all control variables\n- the **explained effect** as the difference between the full and direct effect\n- a **single scalar value** for the individual contributions of a covariate to overall **explained effect** \n\nFor our example at hand, the additional covariates only explain a tiny fraction of the differences in log wages between men and women - 0.064 points. \nOf these, around one third can be attributed to ethnicity, 0.00064 to years of eduaction, etc. \n\nBecause experience is a categorical variable, the table gets pretty unhandy: we produce one estimate for \"each\" level. Luckily, Gelbach's decomposition \nallows us to group individual contributions into a single number. In the `decompose()` method, we can combine variables via the `combine_covariates` argument: \n\nWe now report a single value for \"experience\", which explains a good chunk - around half - of the explained part of the gender wage gap. \n\nWe can aggregate even more to \"individual level\" and \"job\" level variables: \n\n## Literature \n\n- [\"When do Covariates Matter? And Which Ones, and How Much?\" by Gelbach, Jonah B. (2016), Journal of Labor Economics](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1425737)\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"python":"/pyfixest/.pixi/envs/docs/Scripts/python.exe","engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"regression_decomposition.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.32","quartodoc":{"package":"pyfixest","title":"PyFixest Function Reference","parser":"numpy","rewrite_all_pages":false,"sidebar":"_sidebar.yml","sections":[{"title":"Estimation Functions","desc":"User facing estimation functions\n","contents":["estimation.estimation.feols","estimation.estimation.fepois","estimation.estimation.feglm","did.estimation.did2s","did.estimation.lpdid","did.estimation.event_study","estimation.bonferroni","estimation.rwolf"]},{"title":"Estimation Classes","desc":"Details on Methods and Attributes\n","contents":["estimation.feols_.Feols","estimation.fepois_.Fepois","estimation.feiv_.Feiv","estimation.feglm_.Feglm","estimation.felogit_.Felogit","estimation.feprobit_.Feprobit","estimation.fegaussian_.Fegaussian","estimation.feols_compressed_.FeolsCompressed"]},{"title":"Summarize and Visualize","desc":"Post-Processing of Estimation Results\n","contents":["did.visualize.panelview","report.summary","report.etable","report.dtable","report.coefplot","report.iplot","did.visualize.panelview"]},{"title":"Misc / Utilities","desc":"PyFixest internals and utilities\n","contents":["estimation.demean","estimation.detect_singletons","estimation.model_matrix_fixest"]}]},"title":"How much do Covariates Matter?"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}
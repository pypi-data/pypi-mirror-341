import os
import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import transforms
from torch.utils.data import Dataset
from PIL import Image
import torchvision.models as models

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
image_dir = os.path.join(BASE_DIR, "celeba_5000_sample")

# V√©rification de l'appareil GPU
use_gpu = False
device = torch.device("cuda:0" if use_gpu and torch.cuda.is_available() else "cpu")
print(f"Ex√©cution sur {device}")

# Transformation des images
transform = transforms.Compose([
    transforms.Resize((64, 64)),  # Ajuste la taille des images
    transforms.ToTensor(),         # Convertit en tenseur PyTorch
    transforms.Normalize((0.5,), (0.5,))  # Normalisation (pour centrer les valeurs autour de 0)
])

# Dataset personnalis√©
class CustomImageDataset(Dataset):
    def __init__(self, image_dir, transform=None, max_images=None):
        self.image_dir = image_dir
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
        if max_images:
            self.image_files = self.image_files[:max_images]  # Limiter le nombre d'images
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_files[idx])
        image = Image.open(img_path).convert("RGB")  # Force RGB ici
        if self.transform:
            image = self.transform(image)
        return image

# DataLoader
dataset = CustomImageDataset(image_dir, transform=transform)
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

# D√©finition du VAE
latent_dimension = 512
class VAE(nn.Module):
    def __init__(self, latent_dim):
        super(VAE, self).__init__()

        # Encodeur
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1),
            nn.BatchNorm2d(256),
            nn.ReLU()
        )

        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)
        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)

        # D√©codeur
        self.fc_decode = nn.Linear(latent_dim, 256 * 4 * 4)

        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 4, 2, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, 2, 1),
            nn.BatchNorm2d(3),
            nn.Sigmoid()
        )

    def encode(self, x):
        x = self.encoder(x)
        x = x.view(x.size(0), -1)  # Flatten
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        x = self.fc_decode(z)
        x = x.view(x.size(0), 256, 4, 4)
        return self.decoder(x)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

vae = VAE(latent_dimension)

# Mod√®le pr√©-entra√Æn√© pour la perte perceptuelle
vgg = models.vgg16(pretrained=True).features[:16].eval()
for param in vgg.parameters():
    param.requires_grad = False
vgg = vgg.to(device)

# Calcul de la perte perceptuelle
def perceptual_loss(x, y, model):
    x_vgg = model(x)
    y_vgg = model(y)
    return F.l1_loss(x_vgg, y_vgg)

# Fonction de perte du VAE
def loss_function(recon_x, x, mu, logvar, perceptual_weight=1.0):
    pixel_loss = F.mse_loss(recon_x, x, reduction='sum')
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

    # Normalisation pour VGG
    def normalize_for_vgg(tensor):
        mean = torch.tensor([0.485, 0.456, 0.406], device=tensor.device).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225], device=tensor.device).view(1, 3, 1, 1)
        return (tensor - mean) / std

    x_norm = normalize_for_vgg(x)
    recon_x_norm = normalize_for_vgg(recon_x)
    p_loss = perceptual_loss(recon_x_norm, x_norm, vgg)

    return pixel_loss + 0.2 * kl_loss + perceptual_weight * p_loss

# Optimiseur
optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)

vae_path = os.path.join(BASE_DIR, "vae_weights_4.4.0.pth")

# Entra√Ænement (actuellement d√©sactiv√©)
training = False
if training:
    num_epochs = 15
    for epoch in range(num_epochs):
        running_loss = 0.0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for images in pbar:
            images = images.to(device)
            optimizer.zero_grad()

            recon_images, mu, logvar = vae(images)
            loss = loss_function(recon_images, images, mu, logvar)

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            avg_loss = running_loss / (pbar.n + 1)

            pbar.set_description(f"Epoch {epoch+1}: loss = {avg_loss:.3f}")
    torch.save(vae.state_dict(), vae_path)
else:
    vae.load_state_dict(torch.load(vae_path, map_location=device))



vae.eval()

# G√©n√©ration d'images al√©atoires
z = torch.randn(16, latent_dimension).to(device)
generated_images = vae.decode(z).cpu().detach()

# Affichage des images g√©n√©r√©es
# (N.B. L'affichage est optionnel, on l'enl√®ve si n√©cessaire)
# grid = vutils.make_grid(generated_images, nrow=4, normalize=True)
# plt.figure(figsize=(8, 8))
# plt.imshow(grid.permute(1, 2, 0))
# plt.axis("off")
# plt.show()

# Visualisation des reconstructions (optionnel)
# def visualize_reconstructions(net, images, device=device):
#     with torch.no_grad():
#         images = images.to(device)
#         reconstructions = net(images)[0]
#         return make_grid(reconstructions[1:50], 10, 5).cpu()

# Fonction pour extraire les codes latents
def extract_latent_codes(model, dataloader, device):
    model.eval()
    latents = []
    with torch.no_grad():
        for images in dataloader:
            images = images.to(device)
            mu, log_var = model.encode(images)
            latents.append(mu.cpu().numpy())
    latents = np.concatenate(latents, axis=0)
    return latents

# Fonction pour tracer l'espace latent
def plot_latent_space(model, dataloader, device, method="tsne", title="Latent Space"):
    latents = extract_latent_codes(model, dataloader, device)

    if latents.shape[1] > 2:
        if method == "tsne":
            reducer = TSNE(n_components=2, perplexity=30, random_state=42)
        elif method == "pca":
            reducer = PCA(n_components=2)
        latents_2d = reducer.fit_transform(latents)
    else:
        latents_2d = latents

    # Affichage optionnel (d√©sactiv√© ici)
    # plt.figure(figsize=(8, 6))
    # plt.scatter(latents_2d[:, 0], lat


# Utilisation du dataset
# dataset = CustomImageDataset(image_dir, transform=transform, max_images=50000)
dataset = CustomImageDataset(image_dir, transform=transform)
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

import matplotlib.pyplot as plt
import torchvision.utils as vutils

# R√©cup√©rer un batch d'images
images = next(iter(dataloader))


import pandas as pd

# Chemin vers le CSV
ATTR_CSV = os.path.join(BASE_DIR, "list_attr_celeba.csv")

# Chargement du fichier CSV des attributs
attr_df = pd.read_csv(ATTR_CSV, index_col=0)

# Conversion des -1 en 0 pour que tout soit binaire
attr_df[attr_df == -1] = 0

# Exemple d'acc√®s : attr_df.loc['202599.jpg']['Smiling']

###################### ALGO GENETIQUE ##############################

import os
import torch
import pandas as pd
import numpy as np
from torchvision import transforms, utils as vutils
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

# === Param√®tres pour les attributs ===

IMG_DIR = "celeba_5000_sample"

# === Param√®tres pour les autres traitements ===
NB_SAMPLES = 5000  # Nombre d'√©chantillons pour calculer les directions
IMG_SIZE = 64  # Taille de l'image apr√®s redimensionnement
attribute_directions = {}  # Dictionnaire pour stocker les directions des attributs

# === Param√®tres d'entra√Ænement et de g√©n√©ration ===
mutation_strength = 0.5  # Force de la mutation dans la g√©n√©ration
attribute_strength = 1.0  # Force du changement lors du for√ßage d'attributs
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ----- 1. Nettoyer le CSV en enlevant l'extension .jpg -----
df = pd.read_csv(ATTR_CSV, sep=';')

# ----- 2. Encode les images s√©lectionn√©es -----
def get_latents_from_images(images, model, device):
    model.eval()
    with torch.no_grad():
        images = images.to(device)
        mu, _ = model.encode(images)
        return mu.cpu().numpy()

# ----- 3. G√©n√®re 10 enfants par crossover + mutation -----
def generate_initial_population(parents_latents, n_children=10, mutation_strength=0.5):
    children = []
    for _ in range(n_children):
        p1, p2 = np.random.choice(len(parents_latents), 2, replace=True)
        alpha = 0.5
        child = alpha * parents_latents[p1] + (1 - alpha) * parents_latents[p2]
        mutation = np.random.randn(*child.shape) * mutation_strength
        child += mutation
        children.append(child)
    return np.array(children)

# ----- 4. Decode les latents en images -----
def decode_latents_to_images(latents, model, device):
    model.eval()
    with torch.no_grad():
        latents = torch.tensor(latents, dtype=torch.float32).to(device)
        images = model.decode(latents)
        return images.cpu()

# ----- 5. Affichage des images -----
def show_images(images, title="G√©n√©ration actuelle"):
    grid = vutils.make_grid(images, nrow=5, normalize=True)
    plt.figure(figsize=(12, 6))
    plt.title(title)
    plt.imshow(grid.permute(1, 2, 0))
    plt.axis("off")
    plt.show()

# ----- 6. S√©lection de l'utilisateur -----
def ask_user_selection(n=10):
    while True:
        selection = input(f"Quels visages s√©lectionnez-vous ? (1-{n}, 0=reg√©n√©rer, 'undo'=retour) : ").strip()
        if selection.lower() == "undo":
            return "undo"
        elif selection == "0":
            return []
        else:
            try:
                indices = [int(x.strip()) - 1 for x in selection.split()]
                if all(0 <= idx < n for idx in indices) and len(indices) > 0:
                    return indices
                else:
                    print("‚ùå Indices invalides. R√©essaye.")
            except ValueError:
                print("‚ùå Format invalide. R√©essaye.")

# ----- 7. Force des attributs -----
def force_attributes_on_latents(children_latents, attr_list, attribute_directions, attribute_strength=1.0):
    for i in range(len(children_latents)):
        for attr in attr_list:
            children_latents[i] += attribute_directions[attr] * attribute_strength
    return children_latents

# ----- 8. Calcul des directions des attributs depuis le CSV -----
def compute_attribute_directions_from_csv(model, device):
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor()
    ])

    print("üîÑ Chargement et encodage des images CelebA pour calcul des attributs...")

    attr_df = pd.read_csv(ATTR_CSV, sep=';', index_col=0)
    attr_names = list(attr_df.columns)
    selected_df = attr_df.sample(n=NB_SAMPLES, random_state=42)
    images, attrs = [], []

    for img_name, row in tqdm(selected_df.iterrows(), total=NB_SAMPLES):
        img_name_with_extension = img_name.strip()
        path = os.path.join(IMG_DIR, img_name_with_extension)

        if not os.path.exists(path):
            print(f"‚ö†Ô∏è L'image {img_name_with_extension} n'a pas √©t√© trouv√©e dans le dossier.")
            continue

        try:
            img = Image.open(path).convert("RGB")
            img = transform(img)
            images.append(img)
            attrs.append(row.values)
        except Exception as e:
            print(f"‚ö†Ô∏è √âchec de chargement de l'image {img_name_with_extension} : {e}")
            continue

    if len(images) == 0:
        raise RuntimeError("Aucune image valide n'a √©t√© charg√©e.")

    images = torch.stack(images).to(device)

    with torch.no_grad():
        mu, _ = model.encode(images)
        latents = mu.cpu().numpy()

    attrs = np.array(attrs)

    return latents, attrs, attr_names

# ----- 9. Boucle √©volutive manuelle -----
def genetic_manual_loop(initial_images, model, device, mutation_strength=0.5):
    global attribute_directions
    if len(attribute_directions) == 0:
        latents, attrs, attr_names = compute_attribute_directions_from_csv(model, device)

        attribute_directions = {}
        for i, attr in enumerate(attr_names):
            try:
                positive = latents[attrs[:, i] == 1]
                negative = latents[attrs[:, i] == -1]
                if len(positive) > 0 and len(negative) > 0:
                    direction = positive.mean(axis=0) - negative.mean(axis=0)
                    attribute_directions[attr] = direction
                else:
                    print(f"‚ö†Ô∏è Pas assez d'exemples pour l'attribut '{attr}', ignor√©.")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du calcul de la direction pour '{attr}' : {e}")
                continue

        print(f"‚úÖ {len(attribute_directions)} directions d'attributs calcul√©es.")

    parents_latents = get_latents_from_images(initial_images, model, device)
    history = [parents_latents.copy()]
    generation = 1

    while True:
        print(f"\nüîÅ G√©n√©ration {generation}")
        children_latents = generate_initial_population(parents_latents, mutation_strength=mutation_strength)
        children_images = decode_latents_to_images(children_latents, model, device)

        #show_images(children_images, title=f"G√©n√©ration {generation}")

        attr_input = input("\nüì¢ Veux-tu forcer des attributs sur la prochaine g√©n√©ration ?\n"
                           f"Liste dispo : {', '.join(list(attribute_directions.keys())[:10])}...\n"
                           "Entre des attributs s√©par√©s par un espace (ex: Smiling Male), ou ENTER pour rien : ").strip()

        attr_list = [attr for attr in attr_input.split() if attr in attribute_directions]
        if attr_input and not attr_list:
            print("‚ö†Ô∏è Certains attributs sont inconnus ou non support√©s.")
        if attr_list:
            print(f"‚ú® For√ßage des attributs : {', '.join(attr_list)}")
            children_latents = force_attributes_on_latents(children_latents, attr_list, attribute_directions, attribute_strength)


        selection = ask_user_selection(n=10)

        if selection == "undo":
            if generation == 1:
                print("‚ö†Ô∏è Tu es d√©j√† √† la premi√®re g√©n√©ration, impossible de revenir en arri√®re.")
                continue
            history.pop()
            parents_latents = history[-1]
            generation -= 1
            print("‚Ü©Ô∏è Retour √† la g√©n√©ration pr√©c√©dente.")
            continue

        elif selection == []:
            print("üîÅ Nouvelle g√©n√©ration avec les m√™mes parents...")
            continue

        else:
            parents_latents = children_latents[selection]
            history.append(parents_latents.copy())
            generation += 1
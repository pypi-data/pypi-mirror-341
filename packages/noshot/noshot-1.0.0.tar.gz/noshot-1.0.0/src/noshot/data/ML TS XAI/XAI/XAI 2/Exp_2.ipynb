{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdwg19aUY6j6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB  # Importing Naive Bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hrwNktWpZoNM",
    "outputId": "e5640cd0-28f0-4c67-a0cd-b22b22091984"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2,\n",
    "                           class_sep=2, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "print(\"Original class distribution:\", Counter(y)) #counter is used to count the number of occurrences\n",
    "\n",
    "# Plot the class distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(['Class 0', 'Class 1'], [Counter(y)[0], Counter(y)[1]], color=['cyan', 'black'])\n",
    "plt.title('Original Class Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique) for oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Display new class distribution\n",
    "print(\"Resampled class distribution:\", Counter(y_res))\n",
    "\n",
    "# Plot the resampled class distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(['Class 0', 'Class 1'], [Counter(y_res)[0], Counter(y_res)[1]], color=['cyan', 'black'])\n",
    "plt.title('Resampled Class Distribution (SMOTE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Train and evaluate a Naive Bayes classifier on the resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)\n",
    "\n",
    "# Using Naive Bayes Classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_resampled = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the resampled dataset\n",
    "print(\"Classification report on resampled data:\")\n",
    "print(classification_report(y_test, y_pred_resampled))\n",
    "\n",
    "# Confusion Matrix for resampled data\n",
    "cm_resampled = confusion_matrix(y_test, y_pred_resampled)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_resampled, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Resampled Data\")\n",
    "plt.show()\n",
    "\n",
    "# Now, evaluate the model on the original imbalanced data\n",
    "X_train_imbalanced, X_test_imbalanced, y_train_imbalanced, y_test_imbalanced = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes Classifier on imbalanced data\n",
    "clf.fit(X_train_imbalanced, y_train_imbalanced)\n",
    "y_pred_imbalanced = clf.predict(X_test_imbalanced)\n",
    "\n",
    "# Evaluate the model on the original imbalanced dataset\n",
    "print(\"Classification report on imbalanced data:\")\n",
    "print(classification_report(y_test_imbalanced, y_pred_imbalanced))\n",
    "\n",
    "# Confusion Matrix for imbalanced data\n",
    "cm_imbalanced = confusion_matrix(y_test_imbalanced, y_pred_imbalanced)\n",
    "disp_imbalanced = ConfusionMatrixDisplay(confusion_matrix=cm_imbalanced, display_labels=['Class 0', 'Class 1'])\n",
    "disp_imbalanced.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Imbalanced Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "iXasw8cxb8OV",
    "outputId": "eb0e821f-61fd-4a85-c94a-fdcfa55b2ed4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data with a linear relationship (y = 2X + 1)\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100)#generates 100 evenly spaced points in the range from 0 to 10\n",
    "y = 2 * X + 1  # True underlying function\n",
    "\n",
    "# Add some noise to the data\n",
    "noise = np.random.normal(0, 2, X.shape)#take ranom values from a normal distribution of mean 0 and SD 2 same as X.shape\n",
    "y_noisy = y + noise #y is y added with noise\n",
    "\n",
    "# Introduce outliers\n",
    "X_outliers = np.array([2, 4, 6, 8])\n",
    "y_outliers = np.array([25, 30, 28, 35])  # Outliers with large values\n",
    "X_combined = np.concatenate((X, X_outliers))\n",
    "y_combined = np.concatenate((y_noisy, y_outliers))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data to fit into linear regression model\n",
    "X_train = X_train.reshape(-1, 1) #1: Tells NumPy to automatically calculate the number of rows based on the total number of elements\n",
    "\n",
    "# and the specified number of columns (1 in this case).\n",
    "X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on both training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Test data')\n",
    "plt.plot(X_test, y_test_pred, color='cyan', label='2nd Fitted line')\n",
    "plt.plot(X_train, y_train_pred, color='red', label='1st Fitted line (with outliers)',linestyle='-.')\n",
    "plt.plot(X, y, color='black', label='True line (y = 2X + 1)', linestyle='--')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression with Outliers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "fi-j-O-dfIWz",
    "outputId": "81b98378-aff1-436a-e32b-47a5f23929ba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Step 1: Generate synthetic data (initial dataset)\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "X1 = np.random.normal(0, 1, n_samples)  # Feature 1\n",
    "X2 = np.random.normal(0, 1, n_samples)  # Feature 2\n",
    "X = np.column_stack((X1, X2))\n",
    "y = (X1 + X2 > 0).astype(int)  # Binary target\n",
    "\n",
    "# Step 2: Fit logistic regression on the initial data\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Step 3: Simulate drift (change the distribution of features)\n",
    "X1_drifted = np.random.normal(1, 1, n_samples)  # Shift mean from 0 to 1\n",
    "X2_drifted = np.random.normal(1, 1, n_samples)  # Shift mean from 0 to 1\n",
    "X_drifted = np.column_stack((X1_drifted, X2_drifted))\n",
    "y_drifted = (X1_drifted + X2_drifted > 0).astype(int)  # New labels based on drifted data\n",
    "\n",
    "# Step 4: Detect drift using KL divergence\n",
    "def kl_divergence(p, q, bins=10):\n",
    "    \"\"\"Calculate KL divergence between two distributions.\"\"\"\n",
    "    p_hist, _ = np.histogram(p, bins=bins, density=True)\n",
    "    q_hist, _ = np.histogram(q, bins=bins, density=True)\n",
    "    p_hist += 1e-10  # Avoid division by zero\n",
    "    q_hist += 1e-10\n",
    "    return entropy(p_hist, q_hist)\n",
    "\n",
    "# Calculate KL divergence for each feature\n",
    "kl_X1 = kl_divergence(X1, X1_drifted)\n",
    "kl_X2 = kl_divergence(X2, X2_drifted)\n",
    "\n",
    "# Step 5: Evaluate model performance on drifted data\n",
    "y_pred_drifted = model.predict(X_drifted)\n",
    "accuracy_drifted = accuracy_score(y_drifted, y_pred_drifted)\n",
    "log_loss_drifted = log_loss(y_drifted, model.predict_proba(X_drifted))\n",
    "\n",
    "# Step 6: Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot original and drifted distributions\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(X1, bins=20, alpha=0.6, label=\"Feature 1 (original)\", color=\"blue\")\n",
    "plt.hist(X1_drifted, bins=20, alpha=0.6, label=\"Feature 1 (drifted)\", color=\"orange\")\n",
    "plt.title(f\"KL Divergence for Feature 1: {kl_X1:.4f}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(X2, bins=20, alpha=0.6, label=\"Feature 2 (original)\", color=\"blue\")\n",
    "plt.hist(X2_drifted, bins=20, alpha=0.6, label=\"Feature 2 (drifted)\", color=\"orange\")\n",
    "plt.title(f\"KL Divergence for Feature 2: {kl_X2:.4f}\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Print performance metrics\n",
    "print(f\"Model accuracy on drifted data: {accuracy_drifted:.4f}\")\n",
    "print(f\"Log loss on drifted data: {log_loss_drifted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hAnWt1NKfg_c",
    "outputId": "cd7ab20d-b6d8-433c-f6b2-5875068ac0ae"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Simulated dataset\n",
    "data = pd.DataFrame({\n",
    "    'Income': [30000, 45000, 60000, 80000, 20000, 50000],\n",
    "    'CreditScore': [600, 650, 700, 750, 550, 680],\n",
    "    'Gender': [0, 1, 0, 1, 0, 1],  # 0: Male, 1: Female\n",
    "    'Approved': [0, 1, 1, 1, 0, 1]\n",
    "})\n",
    "\n",
    "X = data[['Income', 'CreditScore', 'Gender']]\n",
    "y = data['Approved']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Identifying bias in coefficients\n",
    "print(\"Model Coefficients:\", model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoNlmk5jfikz",
    "outputId": "8b987f2b-983f-48cf-89b5-f203712a144e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Small synthetic dataset\n",
    "X = np.random.rand(100, 1)\n",
    "y = X**2 + np.random.normal(0, 0.05, (100, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test = X[:80], X[80:]\n",
    "y_train, y_test = y[:80], y[80:]\n",
    "\n",
    "# Overfitted Neural Network\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=1),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "\n",
    "# Evaluation\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Train Loss:\", train_loss)\n",
    "print(\"Test Loss:\", test_loss)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook utilities\n",
    "\n",
    "> Utilities for working with notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import inspect\n",
    "import nbformat\n",
    "from typing import Type, Any, Optional, Dict, get_type_hints, get_origin, get_args\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create markdown table from a Pydantic dataclass\n",
    "\n",
    "Especially confenient for Documentation written automatically from the Notebooks by nbdev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _format_type(type_hint: Any) -> str:\n",
    "    \"\"\"Format a type hint into a readable string.\"\"\"\n",
    "    if get_origin(type_hint) is not None:\n",
    "        # Handle generic types like List[str], Optional[int], etc.\n",
    "        origin = get_origin(type_hint)\n",
    "        args = get_args(type_hint)\n",
    "        \n",
    "        if origin is Union:\n",
    "            # Handle Optional (Union[X, None])\n",
    "            if len(args) == 2 and args[1] is type(None):\n",
    "                return f\"Optional[{_format_type(args[0])}]\"\n",
    "            else:\n",
    "                return f\"Union[{', '.join(_format_type(arg) for arg in args)}]\"\n",
    "        \n",
    "        # Handle other generic types\n",
    "        origin_name = origin.__name__ if hasattr(origin, \"__name__\") else str(origin).replace(\"typing.\", \"\")\n",
    "        args_str = \", \".join(_format_type(arg) for arg in args)\n",
    "        return f\"{origin_name}[{args_str}]\"\n",
    "    \n",
    "    # Handle non-generic types\n",
    "    if hasattr(type_hint, \"__name__\"):\n",
    "        return type_hint.__name__\n",
    "    \n",
    "    return str(type_hint).replace(\"typing.\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _escape_table_cell(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape special characters in markdown table cells.\n",
    "    The key is to escape pipe characters with HTML entity or backslash.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Replace pipe characters with HTML entity\n",
    "    # This is the most reliable way to prevent them from being interpreted as column separators\n",
    "    return text.replace(\"|\", \"\\|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pydantic_to_markdown_table(model_class: Type[BaseModel]) -> None:\n",
    "    \"\"\"\n",
    "    Convert a Pydantic model class to a markdown table and display it in Jupyter notebook.\n",
    "    \n",
    "    Args:\n",
    "        model_class: A Pydantic model class (subclass of BaseModel)\n",
    "    \"\"\"\n",
    "    if not issubclass(model_class, BaseModel):\n",
    "        raise TypeError(\"Input must be a Pydantic BaseModel class\")\n",
    "    \n",
    "    md_name = f\"## {model_class.__name__}\\n\"\n",
    "    md_docstring = f\"{inspect.getdoc(model_class)}\\n\" or \"\"\n",
    "    \n",
    "    # Get source code lines to extract comments\n",
    "    try:\n",
    "        source_lines = inspect.getsource(model_class).splitlines()\n",
    "    except (OSError, TypeError):\n",
    "        source_lines = []\n",
    "    \n",
    "    # Extract property comments from source code\n",
    "    property_comments = {}\n",
    "    for i, line in enumerate(source_lines):\n",
    "        if \":\" in line and \"#\" in line:\n",
    "            # Extract property name and comment\n",
    "            property_part = line.split(\":\")[0].strip()\n",
    "            comment_part = line.split(\"#\")[1].strip()\n",
    "            property_comments[property_part] = comment_part\n",
    "    \n",
    "    # Start building the markdown table\n",
    "    table = \"\\n| Variable | Type | Default | Details |\\n\"\n",
    "    table += \"|---|---|---|---|\\n\"\n",
    "    \n",
    "    # Get type hints and model fields\n",
    "    type_hints = get_type_hints(model_class)\n",
    "    \n",
    "    # Handle both Pydantic v1 and v2\n",
    "    model_fields = getattr(model_class, \"model_fields\", None)\n",
    "    if model_fields is None:\n",
    "        model_fields = getattr(model_class, \"__fields__\", {})\n",
    "    \n",
    "    # Process each field\n",
    "    for field_name, field_type in type_hints.items():\n",
    "        # Skip private fields and methods\n",
    "        if field_name.startswith('_'):\n",
    "            continue\n",
    "        \n",
    "        # Get field info\n",
    "        field_info = None\n",
    "        if model_fields and field_name in model_fields:\n",
    "            field_info = model_fields[field_name]\n",
    "        \n",
    "        # Format type string\n",
    "        type_str = _format_type(field_type)\n",
    "        \n",
    "        # Get default value\n",
    "        default_value = \"...\"  # Pydantic's notation for required fields\n",
    "        \n",
    "        # Try to get default from field info\n",
    "        if field_info:\n",
    "            # For Pydantic v2\n",
    "            if hasattr(field_info, \"default\") and field_info.default is not inspect.Signature.empty:\n",
    "                default_value = _escape_table_cell(repr(field_info.default))\n",
    "            # For Pydantic v1\n",
    "            elif hasattr(field_info, \"default\") and not field_info.required:\n",
    "                default_value = _escape_table_cell(repr(field_info.default))\n",
    "        \n",
    "        # Get description\n",
    "        description = \"\"\n",
    "        \n",
    "        # Try to get description from Field\n",
    "        if field_info and hasattr(field_info, \"description\") and field_info.description:\n",
    "            description = _escape_table_cell(field_info.description)\n",
    "        # Fallback to comment\n",
    "        elif field_name in property_comments:\n",
    "            description = _escape_table_cell(property_comments[field_name])\n",
    "        \n",
    "        # For nested Pydantic models, add a reference note\n",
    "        if issubclass(field_type, BaseModel) if isinstance(field_type, type) else False:\n",
    "            description += f\" (see `{field_type.__name__}` table)\"\n",
    "        \n",
    "        # Add row to table\n",
    "        table += f\"| `{field_name}` | `{type_str}` | {default_value} | {description} |\\n\"\n",
    "    \n",
    "    return display(Markdown(md_name + md_docstring + table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyChild(BaseModel):\n",
    "    \"\"\"A simple dataclass model\"\"\"\n",
    "    model_name: str = Field(..., description=\"Name or path of the model to use\") # Name\n",
    "    provider: str = Field(default=\"huggingface\", description=\"Model provider (huggingface, openai, etc)\")\n",
    "    api_key_env_var: Optional[str] = Field(default=None, description=\"Environment variable name for API key\")\n",
    "    api_base_url: Optional[str] = Field(default=None, description=\"Base URL for API reqeuest\")\n",
    "    temperature: float = Field(default=0.7, description=\"Temperature for generation\")\n",
    "\n",
    "class DummyParent(BaseModel):\n",
    "    \"\"\"Main configuration for a chat application\"\"\"\n",
    "    app_name: str = Field(..., description=\"Name of the application\")\n",
    "    description: str = Field(default=\"\", description=\"Description of the application\")\n",
    "    system_prompt: str = Field(..., description=\"System prompt for the LLM\")\n",
    "    model: DummyChild\n",
    "    show_system_prompt: bool = Field(default=True, description=\"Whether to show system prompt in UI\")\n",
    "    show_context: bool = Field(default=True, description=\"Whether to show context in UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## DummyParent\n",
       "Main configuration for a chat application\n",
       "\n",
       "| Variable | Type | Default | Details |\n",
       "|---|---|---|---|\n",
       "| `app_name` | `str` | PydanticUndefined | Name of the application |\n",
       "| `description` | `str` | '' | Description of the application |\n",
       "| `system_prompt` | `str` | PydanticUndefined | System prompt for the LLM |\n",
       "| `model` | `DummyChild` | PydanticUndefined |  (see `DummyChild` table) |\n",
       "| `show_system_prompt` | `bool` | True | Whether to show system prompt in UI |\n",
       "| `show_context` | `bool` | True | Whether to show context in UI |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pydantic_to_markdown_table(DummyParent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tree structure from a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clearly display the structure of a Python dictionary. The output only shows the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_dict_structure(\n",
    "    d: Dict, # The dictionary that will be pretty printed\n",
    "    indent=0 # The indent that is used for subkeys\n",
    "    ) -> str:\n",
    "    for key, value in d.items():\n",
    "        print(\"  \" * indent + f\"├── {key}\")\n",
    "        if isinstance(value, dict):\n",
    "            print_dict_structure(value, indent + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── The Big Lebowski\n",
      "  ├── characters\n",
      "    ├── Dude\n",
      "    ├── Walter\n",
      "    ├── Donny\n",
      "  ├── setting\n",
      "    ├── location\n",
      "    ├── object\n",
      "├── Office Space\n",
      "  ├── characters\n",
      "    ├── Peter\n",
      "    ├── Milton\n",
      "    ├── Lumbergh\n",
      "  ├── setting\n",
      "    ├── company\n",
      "    ├── object\n",
      "├── The Princess Bride\n",
      "  ├── characters\n",
      "    ├── Westley\n",
      "    ├── Inigo\n",
      "  ├── setting\n",
      "    ├── location\n",
      "├── Labyrinth\n",
      "  ├── characters\n",
      "    ├── Sarah\n",
      "    ├── Jareth\n"
     ]
    }
   ],
   "source": [
    "movie_dict = {\n",
    "    \"The Big Lebowski\": {\n",
    "        \"characters\": {\n",
    "            \"Dude\": \"White Russian\",\n",
    "            \"Walter\": \"Vietnam\",\n",
    "            \"Donny\": \"Bowling\"\n",
    "        },\n",
    "        \"setting\": {\n",
    "            \"location\": \"Bowling Alley\",\n",
    "            \"object\": \"Rug\"\n",
    "        }\n",
    "    },\n",
    "    \"Office Space\": {\n",
    "        \"characters\": {\n",
    "            \"Peter\": \"TPS report\",\n",
    "            \"Milton\": \"Red stapler\",\n",
    "            \"Lumbergh\": \"Memos\"\n",
    "        },\n",
    "        \"setting\": {\n",
    "            \"company\": \"Initech\",\n",
    "            \"object\": \"Printer\"\n",
    "        }\n",
    "    },\n",
    "    \"The Princess Bride\": {\n",
    "        \"characters\": {\n",
    "            \"Westley\": \"Farm Boy\",\n",
    "            \"Inigo\": \"Revenge\"\n",
    "        },\n",
    "        \"setting\": {\n",
    "            \"location\": \"Cliffs of Insanity\"\n",
    "        }\n",
    "    },\n",
    "    \"Labyrinth\": {\n",
    "        \"characters\": {\n",
    "            \"Sarah\": \"Labyrinth\",\n",
    "            \"Jareth\": \"Goblin King\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print_dict_structure(movie_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export notebook variables to toml or yaml settings file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to create a YAML file from the contents of a Jupyter Notebook. This helps to create a simple interface to set the parameters of the application, without the need to build a complete GUI or demand of the user to edit YAML files directly without the ease and explanations possible in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def export_ipynb_yaml(\n",
    "    nb_path: Optional[str] = None,\n",
    "    output_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Export the content of the current Jupyter notebook to a YAML file.\n",
    "    \n",
    "    This function reads the content of the notebook where it's being executed,\n",
    "    extracts all level 2 (##) markdown cells as groups, and all code cells as\n",
    "    key-value pairs within those groups. Regular text markdown cells are ignored.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nb_path : str, optional\n",
    "        Path to the notebook file. If None, the function will try to determin the\n",
    "        current notebook path automatically (works in standard Jupyter but may not\n",
    "        work in all environments like VS Code).\n",
    "    output_path : str, optional\n",
    "        Path where the YAML file should be saved. If None, the YAML file will be\n",
    "        saved in the same directory as the notebook with the same name but with\n",
    "        a .yaml extension.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "        # Get the path of the notebook that's currently being executed if not provided\n",
    "    if nb_path is None:\n",
    "        try:\n",
    "            import IPython\n",
    "            ipython = IPython.get_ipython()\n",
    "            # Try different methods to get the notebook path\n",
    "            if hasattr(ipython, 'kernel') and hasattr(ipython.kernel, 'path'):\n",
    "                nb_path = ipython.kernel.path\n",
    "            elif hasattr(ipython, 'ev'):\n",
    "                # Try to get the path from the %notebook magic command history\n",
    "                for line in ipython.history_manager.get_tail(100, include_latest=True):\n",
    "                    if '%notebook' in line[2]:\n",
    "                        parts = line[2].split()\n",
    "                        for i, part in enumerate(parts):\n",
    "                            if part == '%notebook' and i+1 < len(parts):\n",
    "                                nb_path = parts[i+1]\n",
    "                                break\n",
    "        except (ImportError, AttributeError):\n",
    "            pass\n",
    "        \n",
    "        if nb_path is None:\n",
    "            raise ValueError(\n",
    "                \"Could not determine the notebook path automatically. \"\n",
    "                \"Please provide the notebook_path parameter explicitly.\"\n",
    "            )\n",
    "    \n",
    "    # Read the notebook\n",
    "    with open(nb_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    # Initialize variables\n",
    "    yaml_data = {}\n",
    "    current_group = None\n",
    "    \n",
    "    # Process cells\n",
    "    for cell in notebook.cells:\n",
    "        # Process markdown cells to find level 2 headings\n",
    "        if cell.cell_type == 'markdown':\n",
    "            lines = cell.source.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.startswith('## '):\n",
    "                    # Found a level 2 heading, use it as a group\n",
    "                    current_group = line[3:].strip()\n",
    "                    yaml_data[current_group] = {}\n",
    "        \n",
    "        # Process code cells if we have a current group\n",
    "        elif cell.cell_type == 'code' and current_group is not None:\n",
    "            # Process each line in the code cell looking for parameter assignments\n",
    "            code_lines = cell.source.split('\\n')\n",
    "            for line in code_lines:\n",
    "                # Skip empty lines and comments\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Look for parameter assignments (var = value)\n",
    "                if '=' in line:\n",
    "                    # Skip line that calls this export function\n",
    "                    if 'export_ipynb_yaml' in line:\n",
    "                        continue\n",
    "\n",
    "                    parts = line.split('=', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        key = parts[0].strip()\n",
    "                        value_str = parts[1].strip()\n",
    "                        \n",
    "                        # Try to evaluate the value to get proper types\n",
    "                        try:\n",
    "                            # Handle string values that might be quoted\n",
    "                            if (value_str.startswith('\"') and value_str.endswith('\"')) or \\\n",
    "                               (value_str.startswith(\"'\") and value_str.endswith(\"'\")):\n",
    "                                value = value_str[1:-1]  # Remove quotes\n",
    "                            else:\n",
    "                                # Try to evaluate as Python literal\n",
    "                                import ast\n",
    "                                value = ast.literal_eval(value_str)\n",
    "                        except (SyntaxError, ValueError):\n",
    "                            # If evaluation fails, use the string as is\n",
    "                            value = value_str\n",
    "                        \n",
    "                        # Add to YAML data\n",
    "                        yaml_data[current_group][key] = value\n",
    "    \n",
    "    # Determine output path if not provided\n",
    "    if output_path is None:\n",
    "        notebook_path = Path(nb_path)\n",
    "        output_path = notebook_path.with_suffix('.yaml')\n",
    "    \n",
    "    # Write to YAML file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        yaml.dump(yaml_data, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"YAML file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to create a TOML file from the contents of a Jupyter Notebook. This helps to create a simple interface to set the parameters of the application, without the need to build a complete GUI or demand of the user to edit TOML files directly without the ease and explanations possible in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def export_ipynb_toml(\n",
    "    nb_path: Optional[str] = None,\n",
    "    output_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Export the content of the current Jupyter notebook to a TOML file.\n",
    "    \n",
    "    This function reads the content of the notebook where it's being executed,\n",
    "    extracts all level 2 (##) markdown cells as groups, and all parameter assignments\n",
    "    in code cells as key-value pairs within those groups. Regular text markdown cells are ignored.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nb_path : str, optional\n",
    "        Path to the notebook file. If None, the function will try to determin the\n",
    "        current notebook path automatically (works in standard Jupyter but may not\n",
    "        work in all environments like VS Code).\n",
    "    output_path : str, optional\n",
    "        Path where the TOML file should be saved. If None, the TOML file will be\n",
    "        saved in the same directory as the notebook with the same name but with\n",
    "        a .toml extension.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get the path of the notebook that's currently being executed if not provided\n",
    "    if nb_path is None:\n",
    "        try:\n",
    "            import IPython\n",
    "            ipython = IPython.get_ipython()\n",
    "            # Try different methods to get the notebook path\n",
    "            if hasattr(ipython, 'kernel') and hasattr(ipython.kernel, 'path'):\n",
    "                nb_path = ipython.kernel.path\n",
    "            elif hasattr(ipython, 'ev'):\n",
    "                # Try to get the path from the %notebook magic command history\n",
    "                for line in ipython.history_manager.get_tail(100, include_latest=True):\n",
    "                    if '%notebook' in line[2]:\n",
    "                        parts = line[2].split()\n",
    "                        for i, part in enumerate(parts):\n",
    "                            if part == '%notebook' and i+1 < len(parts):\n",
    "                                nb_path = parts[i+1]\n",
    "                                break\n",
    "        except (ImportError, AttributeError):\n",
    "            pass\n",
    "        \n",
    "        if nb_path is None:\n",
    "            raise ValueError(\n",
    "                \"Could not determine the notebook path automatically. \"\n",
    "                \"Please provide the notebook_path parameter explicitly.\"\n",
    "            )\n",
    "    \n",
    "    # Read the notebook\n",
    "    with open(nb_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    # Initialize variables\n",
    "    toml_data = {}\n",
    "    current_group = None\n",
    "    \n",
    "    # Process cells\n",
    "    for cell in notebook.cells:\n",
    "        # Process markdown cells to find level 2 headings\n",
    "        if cell.cell_type == 'markdown':\n",
    "            lines = cell.source.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.startswith('## '):\n",
    "                    # Found a level 2 heading, use it as a group\n",
    "                    current_group = line[3:].strip()\n",
    "                    toml_data[current_group] = {}\n",
    "        \n",
    "        # Process code cells if we have a current group\n",
    "        elif cell.cell_type == 'code' and current_group is not None:\n",
    "            # Process each line in the code cell looking for parameter assignments\n",
    "            code_lines = cell.source.split('\\n')\n",
    "            for line in code_lines:\n",
    "                # Skip empty lines and comments\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                # Look for parameter assignments (var = value)\n",
    "                if '=' in line:\n",
    "                    # Skip lines that call export functions\n",
    "                    if 'export_ipynb_' in line:\n",
    "                        continue\n",
    "                        \n",
    "                    parts = line.split('=', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        key = parts[0].strip()\n",
    "                        value_str = parts[1].strip()\n",
    "                        \n",
    "                        # Try to evaluate the value to get proper types\n",
    "                        try:\n",
    "                            # Handle string values that might be quoted\n",
    "                            if (value_str.startswith('\"') and value_str.endswith('\"')) or \\\n",
    "                               (value_str.startswith(\"'\") and value_str.endswith(\"'\")):\n",
    "                                value = value_str[1:-1]  # Remove quotes\n",
    "                            else:\n",
    "                                # Try to evaluate as Python literal\n",
    "                                import ast\n",
    "                                value = ast.literal_eval(value_str)\n",
    "                        except (SyntaxError, ValueError):\n",
    "                            # If evaluation fails, use the string as is\n",
    "                            value = value_str\n",
    "                        \n",
    "                        # Add to TOML data\n",
    "                        toml_data[current_group][key] = value\n",
    "    \n",
    "    # Determine output path if not provided\n",
    "    if output_path is None:\n",
    "        notebook_path = Path(nb_path)\n",
    "        output_path = notebook_path.with_suffix('.toml')\n",
    "\n",
    "    # Write the TOML file manually to ensure proper formatting\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for section, values in toml_data.items():\n",
    "            f.write(f\"[{section}]\\n\")\n",
    "            for key, value in values.items():\n",
    "                # For headers_to_split_on, we need to preserve the tuple structure\n",
    "                if key == \"headers_to_split_on\" and isinstance(value, list) and all(isinstance(item, tuple) for item in value):\n",
    "                    # Write as a TOML array of arrays\n",
    "                    f.write(f\"{key} = [\\n\")\n",
    "                    for tup in value:\n",
    "                        # Each tuple becomes an array in TOML\n",
    "                        elements = []\n",
    "                        for elem in tup:\n",
    "                            if isinstance(elem, str):\n",
    "                                elements.append(f'\"{elem}\"')\n",
    "                            else:\n",
    "                                elements.append(str(elem))\n",
    "                        f.write(f\"  [{', '.join(elements)}],\\n\")\n",
    "                    f.write(\"]\\n\")\n",
    "                else:\n",
    "                    # For other values, use the standard TOML serialization\n",
    "                    # Convert Python value to TOML-compatible string\n",
    "                    if isinstance(value, str):\n",
    "                        f.write(f'{key} = \"{value}\"\\n')\n",
    "                    elif isinstance(value, list):\n",
    "                        # Format list elements properly\n",
    "                        formatted_list = []\n",
    "                        for item in value:\n",
    "                            if isinstance(item, str):\n",
    "                                formatted_list.append(f'\"{item}\"')\n",
    "                            else:\n",
    "                                formatted_list.append(str(item))\n",
    "                        f.write(f\"{key} = [{', '.join(formatted_list)}]\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{key} = {value}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    print(f\"TOML file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default nbdev code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

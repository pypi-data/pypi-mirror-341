Metadata-Version: 2.4
Name: llmcp
Version: 0.1.2
Summary: A minimal CLI for interacting with LLMs via LiteLLM and MCP
Project-URL: repository, https://github.com/cccntu/llmcp
Author-email: Jonathan Chang <31893406+cccntu@users.noreply.github.com>
License-Expression: MIT
License-File: LICENSE
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Python: >=3.10
Requires-Dist: litellm>=1.61.9
Requires-Dist: mcp>=1.6.0
Requires-Dist: rich>=13.0.0
Requires-Dist: typer>=0.9.0
Provides-Extra: dev
Requires-Dist: pytest-asyncio>=0.21.1; extra == 'dev'
Requires-Dist: pytest>=7.4.0; extra == 'dev'
Requires-Dist: ruff>=0.0.292; extra == 'dev'
Description-Content-Type: text/markdown

# llmcp

A minimal CLI for interacting with LLMs via LiteLLM and MCP.

## Installation

```bash
# Install with pip
pip install llmcp

# Or with uv (recommended)
uv pip install llmcp
```

## Quickstart

```bash
# Install and test locally (recommended)
uv pip install llmcp

# Search for available models
llmcp search 'gemini-2*'

# Serve mcp server
# llmcp serve gemini-2.5-pro-exp-03-25

# Test start a mcp server and call it
llmcp test gpt-4o-mini "What is the capital of France?"
# Example output:
# Response:
# The capital of France is Paris.

# Add a gemini:ask tool to Claude Code
claude mcp add gemini uvx llmcp serve gemini-2.5-pro-exp-03-25
```

##

## MCP details:

- MCP server implements a minimalist `ask` tool. The `ask` tool schema is as follows:
  ```json
  {
    "name": "ask",
    "description": "Send a prompt to the {model_name} model and get a response.",
    "input_schema": {
      "type": "object",
      "properties": {
        "prompt": {
          "type": "string",
          "description": "The prompt to send to the {model_name} model."
        }
      },
      "required": ["prompt"]
    }
  }
  ```
- Maximum token usage by default (automatically uses the maximum available tokens for the model)



## Configuration

Set up your API keys as environment variables:

| Provider   | Environment Variable       |
|------------|---------------------------|
| OpenAI     | `OPENAI_API_KEY`          |
| Anthropic  | `ANTHROPIC_API_KEY`       |
| Gemini     | `GEMINI_API_KEY`          |
| Mistral    | `MISTRAL_API_KEY`         |
| Cohere     | `COHERE_API_KEY`          |
| Groq       | `GROQ_API_KEY`            |

Example:
```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-..."
export GEMINI_API_KEY="..."
export MISTRAL_API_KEY="..."
export COHERE_API_KEY="..."
export GROQ_API_KEY="..."
```

## Acknowledgements

This tool relies on the following libraries:

- [LiteLLM](https://github.com/BerriAI/litellm) for interacting with various LLM APIs.
- [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk) for implementing the Model Context Protocol.

## License

MIT

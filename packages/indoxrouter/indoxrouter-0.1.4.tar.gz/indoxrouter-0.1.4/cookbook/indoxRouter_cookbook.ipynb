{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IndoxRouter Cookbook\n",
        "\n",
        "This cookbook provides comprehensive examples of how to use the IndoxRouter client to interact with various AI providers through a unified API.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the IndoxRouter client and import the necessary modules:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03bc1cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "module_path = os.path.abspath('E:/Codes/indoxRouter/')\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install the IndoxRouter client\n",
        "# !pip install indoxrouter\n",
        "\n",
        "# Import the client and exceptions\n",
        "from indoxRouter import Client\n",
        "\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's initialize the client:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize with API key\n",
        "client = Client(api_key=\"indox_4mfg24GRj_qGaZ2-qXw-mXYqaNqMkyGkG1lncGUrRkA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 1. Chat Completions\n",
        "\n",
        "### Basic Chat Completion\n",
        "\n",
        "Generate a simple chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ec17da",
      "metadata": {},
      "outputs": [],
      "source": [
        "client.get_model_info(provider=\"openai\",model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\"  \n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Tokens:\", response[\"usage\"][\"tokens_total\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Chat Completion with Different Provider\n",
        "\n",
        "Use a different provider for chat completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d530c4ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
        "    ],\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multi-turn Conversation\n",
        "\n",
        "Have a multi-turn conversation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize conversation with system prompt and first user message\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with expertise in programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, I'm learning about APIs. Can you help me?\"}\n",
        "]\n",
        "\n",
        "# Get first response from the model\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Add the assistant's response to the conversation\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 1: {response['data'][:50]}...\")  # Print preview of response\n",
        "\n",
        "# Second turn - asking a specific question\n",
        "conversation.append({\"role\": \"user\", \"content\": \"What's the difference between REST and GraphQL APIs?\"})\n",
        "\n",
        "# Get second response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 2: {response['data'][:50]}...\")\n",
        "\n",
        "# Third turn - follow-up question to test context retention\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Can you give me a simple example of each?\"})\n",
        "\n",
        "# Get third response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "conversation.append({\"role\": \"assistant\", \"content\": response[\"data\"]})\n",
        "print(f\"Response 3: {response['data'][:50]}...\")\n",
        "\n",
        "# Fourth turn - testing memory of previous discussion\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Which one would you recommend for a beginner building a small blog site?\"})\n",
        "\n",
        "# Get fourth response\n",
        "response = client.chat(\n",
        "    messages=conversation,\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "print(f\"Response 4: {response['data'][:50]}...\")\n",
        "\n",
        "pprint(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Chat Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story about a robot in 5 sentences.\"}\n",
        "    ],\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Function Calling\n",
        "\n",
        "Use function calling with OpenAI models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define functions\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    \"description\": \"The temperature unit to use\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n",
        "    ],\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    additional_params={\"functions\": functions, \"function_call\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"Function call response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2. Text Completions\n",
        "\n",
        "### Basic Text Completion\n",
        "\n",
        "Generate a simple text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b3ca0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Once upon a time\",\n",
        "    model=\"deepseek/deepseek-chat\",\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Text Completion with Parameters\n",
        "\n",
        "Use different parameters for text completion:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.completion(\n",
        "    prompt=\"Write a recipe for chocolate cake\",\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Streaming Text Completion\n",
        "\n",
        "Stream the response for a better user experience:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Streaming response:\")\n",
        "for chunk in client.completion(\n",
        "    prompt=\"Explain quantum computing in simple terms\",\n",
        "    model=\"mistral/mistral-large-latest\",\n",
        "    stream=True\n",
        "):\n",
        "    if isinstance(chunk, dict) and \"data\" in chunk:\n",
        "        print(chunk[\"data\"], end=\"\", flush=True)\n",
        "    else:\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "print(\"\\nStreaming complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 3. Embeddings\n",
        "\n",
        "### Single Text Embedding\n",
        "\n",
        "Generate embeddings for a single text:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",  \n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"First 5 values:\", response[\"data\"][0][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Text Embeddings\n",
        "\n",
        "Generate embeddings for multiple texts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.embeddings(\n",
        "    text=[\"Hello, world!\", \"How are you?\", \"IndoxRouter is awesome!\"],\n",
        "    model=\"openai/text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(\"Number of embeddings:\", len(response[\"data\"]))\n",
        "print(\"Dimensions of each embedding:\", len(response[\"data\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Using Different Embedding Models\n",
        "\n",
        "Try different embedding models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cohere embeddings\n",
        "response = client.embeddings(\n",
        "    text=\"Hello, world!\",\n",
        "    model=\"mistral-embed\"\n",
        ")\n",
        "\n",
        "print(\"Mistral embedding dimensions:\", len(response[\"data\"][0]))\n",
        "print(\"First 5 values:\", response[\"data\"][0][:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 4. Image Generation\n",
        "\n",
        "### Basic Image Generation\n",
        "\n",
        "Generate a simple image:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A beautiful sunset over the ocean\",\n",
        "    model=\"openai/dall-e-2\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Multiple Images\n",
        "\n",
        "Generate multiple images:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A futuristic city with flying cars\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    n=2\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(response['data'])} images:\")\n",
        "for i, image in enumerate(response[\"data\"]):\n",
        "    print(f\"Image {i+1} URL: {image['url']}\")\n",
        "\n",
        "# Display the images if in a notebook\n",
        "from IPython.display import Image, display\n",
        "for image in response[\"data\"]:\n",
        "    display(Image(url=image[\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Image Generation with Different Parameters\n",
        "\n",
        "Use different parameters for image generation:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.images(\n",
        "    prompt=\"A photorealistic portrait of a cyberpunk character\",\n",
        "    model=\"openai/dall-e-3\",\n",
        "    size=\"1024x1024\",\n",
        "    quality=\"hd\",\n",
        "    style=\"vivid\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", response[\"data\"][0][\"url\"])\n",
        "print(\"Cost:\", response[\"usage\"][\"cost\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 5. Model Information\n",
        "\n",
        "### List All Models\n",
        "\n",
        "Get information about all available models:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = client.models()\n",
        "\n",
        "print(\"Available providers:\")\n",
        "for provider in models:\n",
        "    print(f\"- {provider['name']} ({provider['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(provider['capabilities'])}\")\n",
        "    print(f\"  Models: {len(provider['models'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### List Models for a Specific Provider\n",
        "\n",
        "Get models for a specific provider:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "openai_models = client.models(\"openai\")\n",
        "\n",
        "print(f\"OpenAI models ({len(openai_models['models'])}):\")\n",
        "for model in openai_models[\"models\"]:\n",
        "    print(f\"- {model['name']} ({model['id']})\")\n",
        "    print(f\"  Capabilities: {', '.join(model['capabilities'])}\")\n",
        "    if \"pricing\" in model:\n",
        "        print(f\"  Pricing: Input ${model['pricing']['input']}/1K tokens, Output ${model['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Get Specific Model Information\n",
        "\n",
        "Get detailed information about a specific model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_info = client.get_model_info(\"openai\", \"gpt-4o-mini\")\n",
        "\n",
        "print(f\"Model: {model_info['name']} ({model_info['id']})\")\n",
        "print(f\"Provider: {model_info['provider']}\")\n",
        "print(f\"Description: {model_info['description']}\")\n",
        "print(f\"Capabilities: {', '.join(model_info['capabilities'])}\")\n",
        "print(f\"Input price: ${model_info['pricing']['input']}/1K tokens\")\n",
        "print(f\"Output price: ${model_info['pricing']['output']}/1K tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 6. Usage Statistics\n",
        "\n",
        "### Get Usage Statistics\n",
        "\n",
        "Get usage statistics for the current user:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "usage = client.get_usage()\n",
        "\n",
        "print(\"Usage statistics:\")\n",
        "print(f\"Total requests: {usage['total_requests']}\")\n",
        "print(f\"Total cost: ${usage['total_cost']}\")\n",
        "print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "\n",
        "print(\"\\nBreakdown by endpoint:\")\n",
        "for endpoint, stats in usage[\"endpoints\"].items():\n",
        "    print(f\"- {endpoint}: {stats['requests']} requests, ${stats['cost']} cost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 7. Error Handling\n",
        "\n",
        "### Handle Different Errors\n",
        "\n",
        "Handle different types of errors gracefully:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to demonstrate error handling\n",
        "def try_request(func, *args, **kwargs):\n",
        "    try:\n",
        "        return func(*args, **kwargs)\n",
        "    except ModelNotFoundError as e:\n",
        "        print(f\"Model not found: {e}\")\n",
        "    except ProviderNotFoundError as e:\n",
        "        print(f\"Provider not found: {e}\")\n",
        "    except InsufficientCreditsError as e:\n",
        "        print(f\"Insufficient credits: {e}\")\n",
        "    except InvalidParametersError as e:\n",
        "        print(f\"Invalid parameters: {e}\")\n",
        "    except RateLimitError as e:\n",
        "        print(f\"Rate limit exceeded: {e}\")\n",
        "    except ProviderError as e:\n",
        "        print(f\"Provider error: {e}\")\n",
        "    except NetworkError as e:\n",
        "        print(f\"Network error: {e}\")\n",
        "    except AuthenticationError as e:\n",
        "        print(f\"Authentication error: {e}\")\n",
        "    except IndoxRouterError as e:\n",
        "        print(f\"IndoxRouter error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example: Model not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/model\"\n",
        ")\n",
        "\n",
        "# Example: Invalid parameters\n",
        "result = try_request(client.chat,\n",
        "    messages=\"This is not a list of messages\",  # Should be a list\n",
        "    model=\"openai/gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Example: Provider not found\n",
        "result = try_request(client.chat,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "    model=\"nonexistent/gpt-4o-mini\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 8. Advanced Usage\n",
        "\n",
        "### Using as a Context Manager\n",
        "\n",
        "Use the client as a context manager to automatically close the session:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with Client(api_key=\"your_api_key\") as client:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Response:\", response[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Combining Different Capabilities\n",
        "\n",
        "Combine different capabilities for more complex use cases:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Generate text and then create an image based on it\n",
        "completion_response = client.completion(\n",
        "    prompt=\"Describe a fantastical creature that has never been seen before.\",\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    max_tokens=200\n",
        ")\n",
        "\n",
        "creature_description = completion_response[\"data\"]\n",
        "print(\"Generated description:\", creature_description)\n",
        "\n",
        "# Now create an image based on the description\n",
        "image_response = client.images(\n",
        "    prompt=f\"A detailed illustration of: {creature_description}\",\n",
        "    model=\"openai/dall-e-3\"\n",
        ")\n",
        "\n",
        "print(\"Image URL:\", image_response[\"data\"][0][\"url\"])\n",
        "\n",
        "# Display the image if in a notebook\n",
        "from IPython.display import Image, display\n",
        "display(Image(url=image_response[\"data\"][0][\"url\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Semantic Search with Embeddings\n",
        "\n",
        "Implement a simple semantic search using embeddings:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define some documents\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fast auburn fox leaps above the sleepy canine.\",\n",
        "    \"IndoxRouter provides a unified API for various AI providers.\",\n",
        "    \"The API allows access to multiple AI models through a single interface.\",\n",
        "    \"Paris is the capital of France and known for the Eiffel Tower.\",\n",
        "    \"Rome is the capital of Italy and home to the Colosseum.\"\n",
        "]\n",
        "\n",
        "# Generate embeddings for all documents\n",
        "embeddings_response = client.embeddings(\n",
        "    text=documents,\n",
        "    model=\"openai/text-embedding-ada-002\"\n",
        ")\n",
        "\n",
        "document_embeddings = embeddings_response[\"data\"]\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Function to find most similar documents\n",
        "def semantic_search(query, document_embeddings, documents, top_n=2):\n",
        "    # Get embedding for the query\n",
        "    query_embedding_response = client.embeddings(\n",
        "        text=query,\n",
        "        model=\"openai/text-embedding-ada-002\"\n",
        "    )\n",
        "    query_embedding = query_embedding_response[\"data\"][0]\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = [\n",
        "        cosine_similarity(query_embedding, doc_embedding)\n",
        "        for doc_embedding in document_embeddings\n",
        "    ]\n",
        "\n",
        "    # Get top N results\n",
        "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "\n",
        "    return [\n",
        "        {\"document\": documents[i], \"similarity\": similarities[i]}\n",
        "        for i in top_indices\n",
        "    ]\n",
        "\n",
        "# Example search\n",
        "results = semantic_search(\"What is IndoxRouter?\", document_embeddings, documents)\n",
        "\n",
        "print(\"Search results:\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"{i+1}. {result['document']} (Similarity: {result['similarity']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Implement a simple RAG system:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the same documents and embeddings from the previous example\n",
        "\n",
        "def rag_query(query, document_embeddings, documents, top_n=2):\n",
        "    # Get relevant documents\n",
        "    relevant_docs = semantic_search(query, document_embeddings, documents, top_n)\n",
        "\n",
        "    # Create a context from the relevant documents\n",
        "    context = \"\\n\".join([doc[\"document\"] for doc in relevant_docs])\n",
        "\n",
        "    # Create a prompt with the context\n",
        "    prompt = f\"\"\"\n",
        "    Context information:\n",
        "    {context}\n",
        "\n",
        "    Based on the context information, please answer the following question:\n",
        "    {query}\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate a response\n",
        "    response = client.chat(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based only on the provided context.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "\n",
        "    return response[\"data\"]\n",
        "\n",
        "# Example RAG query\n",
        "answer = rag_query(\"What does IndoxRouter do?\", document_embeddings, documents)\n",
        "print(\"RAG Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 9. Troubleshooting and Debugging\n",
        "\n",
        "### Enable Debug Mode\n",
        "\n",
        "Enable debug logging to see detailed information about requests and responses:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable debug logging\n",
        "client.enable_debug()\n",
        "\n",
        "# Try a request\n",
        "try:\n",
        "    response = client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "        model=\"openai/gpt-4o-mini\"\n",
        "    )\n",
        "    print(\"Success!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Testing Server Connection\n",
        "\n",
        "Use the `test_connection` method to verify that your server is accessible and properly configured:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the connection to the server\n",
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info['status']}\")\n",
        "\n",
        "if connection_info['status'] == 'connected':\n",
        "    print(f\"Server URL: {connection_info['url']}\")\n",
        "    print(f\"Status code: {connection_info['status_code']}\")\n",
        "    if connection_info['server_info']:\n",
        "        print(f\"Server info: {connection_info['server_info']}\")\n",
        "else:\n",
        "    print(f\"Error: {connection_info['error']}\")\n",
        "    print(f\"Error type: {connection_info['error_type']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "#### \"Resource not found\" Error\n",
        "\n",
        "If you see a \"Resource not found\" error, it usually means one of the following:\n",
        "\n",
        "1. The server is not running. Make sure your IndoxRouter server is up and running:\n",
        "\n",
        "   ```bash\n",
        "   cd indoxRouter_server\n",
        "   python -m main\n",
        "   ```\n",
        "\n",
        "2. The base URL is incorrect. Check the URL in your environment:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Using base URL: {client.base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3d019a",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "3. The API endpoint path is incorrect. The client automatically adds the API version prefix, but you can check the full URL in the debug logs.\n",
        "\n",
        "#### Server Errors (500 Internal Server Error)\n",
        "\n",
        "If you encounter a 500 Internal Server Error:\n",
        "\n",
        "1. Check the server logs for detailed error information:\n",
        "\n",
        "   ```bash\n",
        "   # Look at the server logs\n",
        "   cd indoxRouter_server\n",
        "   tail -f logs/server.log\n",
        "   ```\n",
        "\n",
        "2. Verify that the provider service is available and properly configured on the server.\n",
        "\n",
        "3. Check if your request parameters are valid for the specific model you're using.\n",
        "\n",
        "4. Try with a different model or provider to see if the issue is specific to one provider.\n",
        "\n",
        "5. If you see a \"too many values to unpack\" error, it might be related to how the server parses the model string. The client now automatically formats the model string to be compatible with the server, but you can try different formats:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8737736",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try using a different model format\n",
        "   try:\n",
        "       # First attempt with standard format\n",
        "       response = client.chat(\n",
        "           messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "           model=\"openai/gpt-4o-mini\"\n",
        "       )\n",
        "   except ProviderError as e:\n",
        "       if \"too many values to unpack\" in str(e):\n",
        "           # Try with a different provider/model\n",
        "           response = client.chat(\n",
        "               messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "               model=\"anthropic/claude-3-haiku\"\n",
        "           )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946ff17d",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Authentication Errors\n",
        "\n",
        "If you see an \"Authentication failed\" error:\n",
        "\n",
        "1. Make sure your API key is correct:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First few characters of the API key (for security)\n",
        "print(f\"API key starts with: {client.api_key[:5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check if your API key is valid on the server.\n",
        "\n",
        "#### Connection Errors\n",
        "\n",
        "If you can't connect to the server:\n",
        "\n",
        "1. Use the `test_connection` method to diagnose the issue:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "connection_info = client.test_connection()\n",
        "print(f\"Connection status: {connection_info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Check for firewall or network issues.\n",
        "\n",
        "#### Insufficient Credits\n",
        "\n",
        "If you see an \"Insufficient credits\" error:\n",
        "\n",
        "1. Check your current credit balance:\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "      usage = client.get_usage()\n",
        "      print(f\"Remaining credits: ${usage['remaining_credits']}\")\n",
        "except Exception as e:\n",
        "      print(f\"Error getting usage: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "2. Contact your administrator to add more credits to your account.\n",
        "\n",
        "## 10. Cleanup\n",
        "\n",
        "Don't forget to close the client when you're done:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.close()\n",
        "print(\"Client closed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This cookbook has demonstrated how to use the IndoxRouter client to interact with various AI providers through a unified API. You can now use these examples as a starting point for your own applications.\n",
        "\n",
        "For more information, refer to the [IndoxRouter documentation](https://docs.indoxrouter.com).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

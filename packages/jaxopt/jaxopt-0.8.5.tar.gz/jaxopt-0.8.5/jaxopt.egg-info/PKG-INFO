Metadata-Version: 2.4
Name: jaxopt
Version: 0.8.5
Summary: Hardware accelerated, batchable and differentiable optimizers in JAX.
Home-page: https://github.com/google/jaxopt
Author: Google LLC
Author-email: no-reply@google.com
License: Apache 2.0
Keywords: optimization,root finding,implicit differentiation,jax
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: jax>=0.2.18
Requires-Dist: jaxlib>=0.1.69
Requires-Dist: numpy>=1.18.4
Requires-Dist: scipy>=1.0.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: summary

# JAXopt

[**Status**](#status)
| [**Installation**](#installation)
| [**Documentation**](https://jaxopt.github.io)
| [**Examples**](https://github.com/google/jaxopt/tree/main/examples)
| [**Cite us**](#citeus)

Hardware accelerated, batchable and differentiable optimizers in
[JAX](https://github.com/google/jax).

- **Hardware accelerated:** our implementations run on GPU and TPU, in addition
  to CPU.
- **Batchable:** multiple instances of the same optimization problem can be
  automatically vectorized using JAX's vmap.
- **Differentiable:** optimization problem solutions can be differentiated with
  respect to their inputs either implicitly or via autodiff of unrolled
  algorithm iterations.

## Status<a id="status"></a>

JAXopt is no longer maintained nor developed. Alternatives may be found on the
JAX [website](https://docs.jax.dev/en/latest/). Some of its features (like
losses, projections, lbfgs optimizer) have been ported into
[optax](https://github.com/google-deepmind/optax). We are sincerely grateful for
all the community contributions the project has garnered over the years.

## Installation<a id="installation"></a>

To install the latest release of JAXopt, use the following command:

```bash
$ pip install jaxopt
```

To install the **development** version, use the following command instead:

```bash
$ pip install git+https://github.com/google/jaxopt
```

Alternatively, it can be installed from sources with the following command:

```bash
$ python setup.py install
```

## Cite us<a id="citeus"></a>

Our implicit differentiation framework is described in this
[paper](https://arxiv.org/abs/2105.15183). To cite it:

```
@article{jaxopt_implicit_diff,
  title={Efficient and Modular Implicit Differentiation},
  author={Blondel, Mathieu and Berthet, Quentin and Cuturi, Marco and Frostig, Roy 
    and Hoyer, Stephan and Llinares-L{\'o}pez, Felipe and Pedregosa, Fabian 
    and Vert, Jean-Philippe},
  journal={arXiv preprint arXiv:2105.15183},
  year={2021}
}
```

## Disclaimer

JAXopt was an open source project maintained by a dedicated team in Google
Research. It is not an official Google product.

Metadata-Version: 2.1
Name: loco-mujoco
Version: 1.0.0
Summary: Imitation learning benchmark focusing on complex locomotion tasks using MuJoCo.
Home-page: https://github.com/robfiras/loco-mujoco
Author: Firas Al-Hafez
Author-email: Firas Al-Hafez <fi.alhafez@gmail.com>
Maintainer-email: Firas Al-Hafez <fi.alhafez@gmail.com>
License: MIT License
        
        Copyright (c) 2024 Al-Hafez
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: Homepage, https://github.com/robfiras/loco-mujoco
Project-URL: Repository, https://github.com/robfiras/loco-mujoco
Project-URL: Issues, https://github.com/robfiras/loco-mujoco/issues
Keywords: Imitation Learning,Reinforcement Learning,Locomotion
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy<2.0
Requires-Dist: scipy>=1.14.0
Requires-Dist: mujoco==3.2.7
Requires-Dist: mujoco-mjx==3.2.7
Requires-Dist: hydra-core
Requires-Dist: jax
Requires-Dist: flax
Requires-Dist: orbax
Requires-Dist: distrax
Requires-Dist: metrx
Requires-Dist: gymnasium
Requires-Dist: pyyaml
Requires-Dist: opencv-python
Requires-Dist: wget
Requires-Dist: gitpython
Requires-Dist: datasets
Requires-Dist: huggingface_hub

<p align="center">
  <img width="70%" src="https://github.com/robfiras/loco-mujoco/assets/69359729/bd2a219e-ddfd-4355-8024-d9af921fb92a">
</p>

![continous integration](https://github.com/robfiras/loco-mujoco/actions/workflows/continuous_integration.yml/badge.svg?branch=dev)
[![Documentation Status](https://readthedocs.org/projects/loco-mujoco/badge/?version=latest)](https://loco-mujoco.readthedocs.io/en/latest/?badge=latest)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI](https://img.shields.io/pypi/v/loco-mujoco)](https://pypi.org/project/loco-mujoco/)
[![Join our Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?style=flat&logo=discord&logoColor=white)](https://discord.gg/gEqR3xCVdn)

> ðŸš€ **Latest News:**
> A **major release (v1.0)** just dropped! ðŸŽ‰  
> LocoMuJoCo now supports MJX and comes with new Jax algorithms. We also added many new environments and +22k datasets! ðŸš€   


**LocoMuJoCo** is an **imitation learning benchmark** specifically designed for **whole-body control**.  
It features a diverse set of environments, including **quadrupeds**, **humanoids**, and **(musculo-)skeletal human models**,
each provided with comprehensive datasets (over 22,000 samples per humanoid).

Although primarily focused on imitation learning, LocoMuJoCo also supports custom reward function classes,  
making it suitable for pure reinforcement learning as well.

<div align="center">
  <img src="imgs/main_lmj.gif"/>
</div>

### Key Advantages 
âœ… Supports **MuJoCo** (single environment) and **MJX** (parallel environments) \
âœ… Includes **12 humanoid and 4 quadruped environments**, featuring 4 **biomechanical human models** \
âœ… Clean single-file JAX algorithms for quick benchmarking (**PPO**, **GAIL**, **AMP**, **DeepMimic**)\
âœ… Combined training and environment into one JITâ€‘compiled function for lightningâ€‘fast training ðŸš€ \
âœ… **Over 22,000 motion capture datasets** (AMASS, LAFAN1, native LocoMuJoCo) retargeted for each humanoid \
âœ… **Robot-to-robot retargeting** allows to retarget any existing dataset from one robot to another \
âœ… Powerful **trajectory comparison metrics** including dynamic time warping and discrete FrÃ©chet distance, all in JAX \
âœ… Interface for Gymnasium \
âœ… Built-in **domain and terrain randomization** \
âœ… Modular design: define, swap, and reuse components like observation types, reward functions, terminal state handlers, and domain randomization \
âœ… [Documentation](https://loco-mujoco.readthedocs.io/)

---

## Installation

You have the choice to install the latest release via PyPI by running 


```bash

pip install loco-mujoco 

```

Or, clone this repo and do an editable installation:

```bash
cd loco-mujoco
pip install -e . 
```

By default, both will install the CPU-version of Jax. If you want to use Jax on the GPU, you need to install the following:

```bash
pip install jax["cuda12"]
````

> [!NOTE]
> If you want to run the **MyoSkeleton** environment, you need to additionally run
> `loco-mujoco-myomodel-init` to accept the license and download the model.


### Datasets

LocoMuJoCo provides three sources of motion capture (mocap) data for humanoid environments: default (provided by us), LAFAN1, and AMASS. The first two datasets
are available on the [LocoMujoCo HuggingFace dataset repository](https://huggingface.co/datasets/robfiras/loco-mujoco-datasets)
and will downloaded and cached automatically for you. AMASS needs to be downloaded and installed separately due to
their licensing. See [here](loco_mujoco/smpl) for more information about the installation.

This is how you can visualize the datasets:

```python
from loco_mujoco.task_factories import ImitationFactory, LAFAN1DatasetConf, DefaultDatasetConf, AMASSDatasetConf


# # example --> you can add as many datasets as you want in the lists!
env = ImitationFactory.make("UnitreeH1",
                            default_dataset_conf=DefaultDatasetConf(["squat"]),
                            lafan1_dataset_conf=LAFAN1DatasetConf(["dance2_subject4", "walk1_subject1"]),
                            # if SMPL and AMASS are installed, you can use the following:
                            #amass_dataset_conf=AMASSDatasetConf(["DanceDB/DanceDB/20120911_TheodorosSourmelis/Capoeira_Theodoros_v2_C3D_poses"])
                            )

env.play_trajectory(n_episodes=3, n_steps_per_episode=500, render=True)
```

#### Speeding up Dataset Loading
LocoMuJoCo only stores datasets with joint positions and velocities to save memory. All other attributes are calculated 
using forward kinematics upon loading. If you want to speed up the dataset loading, you can define caches for the datasets. This will
store the forward kinematics results in a cache file, which will be loaded on the next run: 

```bash
loco-mujoco-set-all-caches --path <path to cache>
```

For instance, you could run:
```bash
loco-mujoco-set-all-caches --path "$HOME/.loco-mujoco-caches"
````

---

## Environments 
You want a quick overview of all **environments** available? You can find it 
[here](/loco_mujoco/environments) and more detailed in the [Documentation](https://loco-mujoco.readthedocs.io/).

<div align="center">
  <img src="imgs/lmj_envs.gif"/>
</div>

And stay tuned! There are many more to come ...

---

## Tutorials

We provide a set of tutorials to help you get started with LocoMuJoCo. You can find them in the [tutorials folder](./examples/tutorials)
or with more explanation in the [documentation](https://loco-mujoco.readthedocs.io/).

If you want to check out training examples of a PPO, GAIL, AMP, or DeepMimic agent, you can find them 
in the [training examples folder](./examples/training_examples). For instance, [here](./examples/training_examples/jax_rl_mimic) is an example of a DeepMimic agent
you can train to achieve a human-like walking in all directions, which was trained in 36 min on an RTX 3080 Ti:

<div align="center">
  <img src="imgs/unitree_h1_walk_anydir.gif"/>
</div>

---
## Citation
```
@inproceedings{alhafez2023b,
title={LocoMuJoCo: A Comprehensive Imitation Learning Benchmark for Locomotion},
author={Firas Al-Hafez and Guoping Zhao and Jan Peters and Davide Tateo},
booktitle={6th Robot Learning Workshop, NeurIPS},
year={2023}
}
```





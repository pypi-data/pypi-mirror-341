# SilentÂ Killers  
### An Exploratory Audit of Exceptionâ€‘Handling in LLMâ€‘Generated Python
![CI](https://github.com/yourâ€‘org/llm-exception-audit/actions/workflows/ci.yml/badge.svg)
![license](https://img.shields.io/badge/license-MIT-blue)

> **tl;dr**â€ƒWe show that largeâ€‘language models often add `try/except`
> blocks that *silently swallow* errors.  Our ASTâ€‘based metric pipeline
> lets anyone quantify that risk across thousands of generated scripts
> in seconds.

---


## 1Â Â Scope of this study

Modern LLMs can write Python that â€œrunsâ€, but *how* it fails matters.
A **bare** `except:` or a blanket `exceptÂ Exception:` with no
reâ€‘raise can mask fatal bugs, leading to silent data corruption or
debugging nightmaresâ€”these are the **silentÂ killers**.

We collected **5 seeds Ã—Â 8 models Ã—Â 3 prompts** (easyÂ â†’Â hard rewrite
tasks) and asked:

* How often do models inject `try/except` at all?  
* Of those, how many are â€œbadâ€ under a strict reâ€‘raise rule?  
* Does difficulty exacerbate the problem?

The full paper is in `docs/` (LaTeX source) and the main plots live in
[`data/figures/`](data/figures).

---

## 2Â Â Repository layout

```
repo-root/
â”œâ”€ src/
â”‚   â””â”€ llm_exception_audit/        â† **reusable package**
â”‚        â”œâ”€ __init__.py
â”‚        â”œâ”€ metrics.py             (AST visitors & regex metrics)
â”‚        â””â”€ cli/
â”‚             â”œâ”€ process_files.py
â”‚             â””â”€ post_processing.py
â”‚
â”œâ”€ data/                           â† studyâ€‘specific artefacts
â”‚   â”œâ”€ propagation_prompt/
â”‚   â”œâ”€ calibration_prompt/
â”‚   â”œâ”€ optimization_prompt{,2}/
â”‚   â””â”€ figures/
â”œâ”€ tests/
â”‚   â””â”€ test_exception_labels.py
â”œâ”€ pyproject.toml
â””â”€ README.md
```

*Everything under `src/llm_exception_audit/` is published to PyPI;*
`data/` stays in the repo (or GitÂ LFS) but is not shipped inside the
wheel.

---

## 3Â Â Installation

```bash
git clone https://github.com/yourâ€‘org/llm-exception-audit.git
cd llm-exception-audit
python -m pip install --upgrade pip
pip install -e .[dev]          # runtime + pytest + ruff
```

> **Requires PythonÂ â‰¥Â 3.9**  
> Runtime deps: `pandas`, `numpy`, `matplotlib`

---

## 4Â Â QuickÂ start

### 4.1Â Â Generate metrics CSVs

```bash
process_files --base-dir data/propagation_prompt
process_files --base-dir data/calibration_prompt
process_files --base-dir data/optimization_prompt
process_files --base-dir data/optimization_prompt2
```

Each run creates

```
data/<prompt_dir>/
    llm_code_metrics.csv
    llm_response_metrics.csv
```

### 4.2Â Â Plots & summary tables

```bash
post_processing --root data
```

Creates:

```
plots_grid_refactored/
    grid_status_3color.png
    grid_loc_continuous.png
    grid_bad_exception_rate.png
    grid_bad_exception_count.png
    bar_parsed_ok_by_difficulty.png
    summary_by_model.csv
    summary_by_difficulty.csv
```

<details>
<summary>Example output</summary>

| codeâ€‘status | badâ€‘rate heatmap |
|-------------|------------------|
| <img src="data/figures/grid_status_3color.png" width="380"> | <img src="data/figures/grid_bad_exception_rate.png" width="380"> |

</details>

### 4.3Â Â Library usage

```python
from llm_exception_audit import code_metrics

python_code = "try:\n    1/0\nexcept Exception:\n    pass"
for metric in code_metrics(python_code):
    print(metric.name, metric.value)
```

---

## 5Â Â Metrics at a glance

| metric | description |
|--------|-------------|
| `exception_handling_blocks` | count of `except` clauses |
| `bad_exception_blocks` | bare `except:` **or** `except Exception:` *without* `raise` |
| `bad_exception_rate` | `bad / total`, 2Â dp |
| `uses_traceback` | calls `traceback.print_exc()` / `.format_exc()` |
| â€¦ | see `src/llm_exception_audit/metrics.py` |

---

## 6Â Â Key pilot finding

> **When a model adds *any* error handling, 50â€“100Â % of those handlers
> are unsafe.**  
> Inclusive badâ€‘rates look tame (0Â â€“Â 0.6) but conditional badâ€‘rates
> (`only_with_try`) spike to **1.0** for several models on simple
> prompts.

---

## 7Â Â Development

```bash
ruff check .          # lint
pytest                # run unit tests
coverage run -m pytest && coverage html
```

CI runs on GitHubÂ Actions across PythonÂ 3.9â€‘3.11 (see `.github/workflows/ci.yml`).

---

## 8Â Â Roadmap

* ðŸš§ dynamic execution traces (runtime errors, coverage)  
* ðŸš§ extend to other unsafe patterns (weak crypto, insecure I/O)  
* ðŸš§ publish TestPyPI wheel

PRs & issues welcome!

---

## 9Â Â License & citation

MITÂ License.  
If you use the metrics or figures, please cite:

```bibtex
@misc{Quick2025SilentKillers,
  title  = {Silent Killers: An Exploratory Audit of Exceptionâ€‘Handling in LLMâ€‘Generated Python},
  author = {Julian Quick},
  year   = {2025},
  url    = {https://github.com/yourâ€‘org/llm-exception-audit}
}
```

*Happy auditingÂ â€“Â donâ€™t let silent errors slip through!*
```

